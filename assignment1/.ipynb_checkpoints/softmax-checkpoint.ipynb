{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](https://deep-learning-su.github.io/assignment-requirements/) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deep_learning_su.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'deep_learning_su/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **deep_learning_su/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.302647\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file deep_learning_su/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from deep_learning_su.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** With random initialization of the weights, the prediction accuracy should be approximately 10%, which is 0.1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n",
      "numerical: 0.002556 analytic: 0.002551, relative error: 1.010646e-03\n",
      "numerical: 0.017193 analytic: 0.017171, relative error: 6.502342e-04\n",
      "numerical: -0.004380 analytic: -0.004344, relative error: 4.141320e-03\n",
      "numerical: 0.010708 analytic: 0.010758, relative error: 2.359210e-03\n",
      "numerical: -0.023138 analytic: -0.023138, relative error: 1.971972e-09\n",
      "numerical: -0.006968 analytic: -0.006934, relative error: 2.434954e-03\n",
      "numerical: -0.000656 analytic: -0.000658, relative error: 1.214483e-03\n",
      "numerical: -0.007659 analytic: -0.007633, relative error: 1.670350e-03\n",
      "numerical: 0.000309 analytic: 0.000288, relative error: 3.400170e-02\n",
      "numerical: 0.006993 analytic: 0.007035, relative error: 3.035717e-03\n",
      "check\n",
      "numerical: -0.009593 analytic: -0.009614, relative error: 1.079200e-03\n",
      "numerical: 0.009458 analytic: 0.009458, relative error: 2.307978e-09\n",
      "numerical: -0.036673 analytic: -0.036675, relative error: 2.134740e-05\n",
      "numerical: -0.012530 analytic: -0.012512, relative error: 7.104064e-04\n",
      "numerical: 0.009683 analytic: 0.009663, relative error: 1.059032e-03\n",
      "numerical: 0.013197 analytic: 0.013176, relative error: 8.049606e-04\n",
      "numerical: 0.027180 analytic: 0.027243, relative error: 1.161015e-03\n",
      "numerical: 0.007238 analytic: 0.007281, relative error: 2.942494e-03\n",
      "numerical: -0.019014 analytic: -0.019014, relative error: 9.200676e-10\n",
      "numerical: 0.016373 analytic: 0.016410, relative error: 1.112373e-03\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from deep_learning_su.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.302647e+00 computed in 0.093376s\n",
      "vectorized loss: 2.302647e+00 computed in 0.050399s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.006819\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from deep_learning_su.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rates  [3e-08, 5e-07, 1e-06, 5e-06, 1e-05, 5e-05, 0.0005, 0.001, 0.005]\n",
      "iteration 0 / 7000: loss 32.612409\n",
      "iteration 100 / 7000: loss 32.250791\n",
      "iteration 200 / 7000: loss 31.893193\n",
      "iteration 300 / 7000: loss 31.540958\n",
      "iteration 400 / 7000: loss 31.192227\n",
      "iteration 500 / 7000: loss 30.845899\n",
      "iteration 600 / 7000: loss 30.505036\n",
      "iteration 700 / 7000: loss 30.168070\n",
      "iteration 800 / 7000: loss 29.839234\n",
      "iteration 900 / 7000: loss 29.507829\n",
      "iteration 1000 / 7000: loss 29.185735\n",
      "iteration 1100 / 7000: loss 28.862974\n",
      "iteration 1200 / 7000: loss 28.546688\n",
      "iteration 1300 / 7000: loss 28.231903\n",
      "iteration 1400 / 7000: loss 27.925424\n",
      "iteration 1500 / 7000: loss 27.619089\n",
      "iteration 1600 / 7000: loss 27.318550\n",
      "iteration 1700 / 7000: loss 27.018581\n",
      "iteration 1800 / 7000: loss 26.721881\n",
      "iteration 1900 / 7000: loss 26.431029\n",
      "iteration 2000 / 7000: loss 26.142023\n",
      "iteration 2100 / 7000: loss 25.856612\n",
      "iteration 2200 / 7000: loss 25.577945\n",
      "iteration 2300 / 7000: loss 25.301322\n",
      "iteration 2400 / 7000: loss 25.028836\n",
      "iteration 2500 / 7000: loss 24.755176\n",
      "iteration 2600 / 7000: loss 24.487355\n",
      "iteration 2700 / 7000: loss 24.224391\n",
      "iteration 2800 / 7000: loss 23.962537\n",
      "iteration 2900 / 7000: loss 23.705807\n",
      "iteration 3000 / 7000: loss 23.448384\n",
      "iteration 3100 / 7000: loss 23.196697\n",
      "iteration 3200 / 7000: loss 22.947854\n",
      "iteration 3300 / 7000: loss 22.701414\n",
      "iteration 3400 / 7000: loss 22.455105\n",
      "iteration 3500 / 7000: loss 22.216729\n",
      "iteration 3600 / 7000: loss 21.978156\n",
      "iteration 3700 / 7000: loss 21.743648\n",
      "iteration 3800 / 7000: loss 21.513726\n",
      "iteration 3900 / 7000: loss 21.281889\n",
      "iteration 4000 / 7000: loss 21.058834\n",
      "iteration 4100 / 7000: loss 20.834750\n",
      "iteration 4200 / 7000: loss 20.610624\n",
      "iteration 4300 / 7000: loss 20.392908\n",
      "iteration 4400 / 7000: loss 20.177752\n",
      "iteration 4500 / 7000: loss 19.964126\n",
      "iteration 4600 / 7000: loss 19.753349\n",
      "iteration 4700 / 7000: loss 19.545536\n",
      "iteration 4800 / 7000: loss 19.340039\n",
      "iteration 4900 / 7000: loss 19.137395\n",
      "iteration 5000 / 7000: loss 18.936975\n",
      "iteration 5100 / 7000: loss 18.736678\n",
      "iteration 5200 / 7000: loss 18.543364\n",
      "iteration 5300 / 7000: loss 18.347849\n",
      "iteration 5400 / 7000: loss 18.156794\n",
      "iteration 5500 / 7000: loss 17.968222\n",
      "iteration 5600 / 7000: loss 17.779779\n",
      "iteration 5700 / 7000: loss 17.597307\n",
      "iteration 5800 / 7000: loss 17.411167\n",
      "iteration 5900 / 7000: loss 17.233299\n",
      "iteration 6000 / 7000: loss 17.054870\n",
      "iteration 6100 / 7000: loss 16.877943\n",
      "iteration 6200 / 7000: loss 16.705337\n",
      "iteration 6300 / 7000: loss 16.533338\n",
      "iteration 6400 / 7000: loss 16.363550\n",
      "iteration 6500 / 7000: loss 16.195359\n",
      "iteration 6600 / 7000: loss 16.031184\n",
      "iteration 6700 / 7000: loss 15.868027\n",
      "iteration 6800 / 7000: loss 15.705695\n",
      "iteration 6900 / 7000: loss 15.546100\n",
      "new acc  0.105\n",
      "lr, rs 3e-08 1000.0\n",
      "iteration 0 / 7000: loss 63.578742\n",
      "iteration 100 / 7000: loss 62.124226\n",
      "iteration 200 / 7000: loss 60.705792\n",
      "iteration 300 / 7000: loss 59.320865\n",
      "iteration 400 / 7000: loss 57.968286\n",
      "iteration 500 / 7000: loss 56.648190\n",
      "iteration 600 / 7000: loss 55.358675\n",
      "iteration 700 / 7000: loss 54.101950\n",
      "iteration 800 / 7000: loss 52.873286\n",
      "iteration 900 / 7000: loss 51.675539\n",
      "iteration 1000 / 7000: loss 50.503749\n",
      "iteration 1100 / 7000: loss 49.361990\n",
      "iteration 1200 / 7000: loss 48.243729\n",
      "iteration 1300 / 7000: loss 47.154365\n",
      "iteration 1400 / 7000: loss 46.089176\n",
      "iteration 1500 / 7000: loss 45.052305\n",
      "iteration 1600 / 7000: loss 44.040160\n",
      "iteration 1700 / 7000: loss 43.049934\n",
      "iteration 1800 / 7000: loss 42.079872\n",
      "iteration 1900 / 7000: loss 41.140943\n",
      "iteration 2000 / 7000: loss 40.218099\n",
      "iteration 2100 / 7000: loss 39.316858\n",
      "iteration 2200 / 7000: loss 38.440938\n",
      "iteration 2300 / 7000: loss 37.586940\n",
      "iteration 2400 / 7000: loss 36.749058\n",
      "iteration 2500 / 7000: loss 35.931167\n",
      "iteration 2600 / 7000: loss 35.133499\n",
      "iteration 2700 / 7000: loss 34.352860\n",
      "iteration 2800 / 7000: loss 33.591843\n",
      "iteration 2900 / 7000: loss 32.851093\n",
      "iteration 3000 / 7000: loss 32.126866\n",
      "iteration 3100 / 7000: loss 31.420252\n",
      "iteration 3200 / 7000: loss 30.728379\n",
      "iteration 3300 / 7000: loss 30.054804\n",
      "iteration 3400 / 7000: loss 29.397169\n",
      "iteration 3500 / 7000: loss 28.754603\n",
      "iteration 3600 / 7000: loss 28.126454\n",
      "iteration 3700 / 7000: loss 27.513896\n",
      "iteration 3800 / 7000: loss 26.918399\n",
      "iteration 3900 / 7000: loss 26.333177\n",
      "iteration 4000 / 7000: loss 25.763228\n",
      "iteration 4100 / 7000: loss 25.207115\n",
      "iteration 4200 / 7000: loss 24.663973\n",
      "iteration 4300 / 7000: loss 24.132412\n",
      "iteration 4400 / 7000: loss 23.614899\n",
      "iteration 4500 / 7000: loss 23.109839\n",
      "iteration 4600 / 7000: loss 22.616485\n",
      "iteration 4700 / 7000: loss 22.135329\n",
      "iteration 4800 / 7000: loss 21.664420\n",
      "iteration 4900 / 7000: loss 21.205869\n",
      "iteration 5000 / 7000: loss 20.758411\n",
      "iteration 5100 / 7000: loss 20.318036\n",
      "iteration 5200 / 7000: loss 19.892500\n",
      "iteration 5300 / 7000: loss 19.475202\n",
      "iteration 5400 / 7000: loss 19.068422\n",
      "iteration 5500 / 7000: loss 18.670839\n",
      "iteration 5600 / 7000: loss 18.281006\n",
      "iteration 5700 / 7000: loss 17.902387\n",
      "iteration 5800 / 7000: loss 17.533185\n",
      "iteration 5900 / 7000: loss 17.171060\n",
      "iteration 6000 / 7000: loss 16.818843\n",
      "iteration 6100 / 7000: loss 16.475328\n",
      "iteration 6200 / 7000: loss 16.139363\n",
      "iteration 6300 / 7000: loss 15.809560\n",
      "iteration 6400 / 7000: loss 15.489051\n",
      "iteration 6500 / 7000: loss 15.177132\n",
      "iteration 6600 / 7000: loss 14.871608\n",
      "iteration 6700 / 7000: loss 14.573802\n",
      "iteration 6800 / 7000: loss 14.282355\n",
      "iteration 6900 / 7000: loss 13.998547\n",
      "new acc  0.089\n",
      "lr, rs 3e-08 2000.0\n",
      "iteration 0 / 7000: loss 457.613133\n",
      "iteration 100 / 7000: loss 382.578820\n",
      "iteration 200 / 7000: loss 319.909937\n",
      "iteration 300 / 7000: loss 267.568488\n",
      "iteration 400 / 7000: loss 223.854893\n",
      "iteration 500 / 7000: loss 187.344009\n",
      "iteration 600 / 7000: loss 156.847892\n",
      "iteration 700 / 7000: loss 131.381045\n",
      "iteration 800 / 7000: loss 110.107787\n",
      "iteration 900 / 7000: loss 92.343313\n",
      "iteration 1000 / 7000: loss 77.504750\n",
      "iteration 1100 / 7000: loss 65.110264\n",
      "iteration 1200 / 7000: loss 54.760351\n",
      "iteration 1300 / 7000: loss 46.115324\n",
      "iteration 1400 / 7000: loss 38.895213\n",
      "iteration 1500 / 7000: loss 32.864903\n",
      "iteration 1600 / 7000: loss 27.828897\n",
      "iteration 1700 / 7000: loss 23.621287\n",
      "iteration 1800 / 7000: loss 20.107820\n",
      "iteration 1900 / 7000: loss 17.173834\n",
      "iteration 2000 / 7000: loss 14.723395\n",
      "iteration 2100 / 7000: loss 12.676185\n",
      "iteration 2200 / 7000: loss 10.966881\n",
      "iteration 2300 / 7000: loss 9.539024\n",
      "iteration 2400 / 7000: loss 8.346300\n",
      "iteration 2500 / 7000: loss 7.350335\n",
      "iteration 2600 / 7000: loss 6.518551\n",
      "iteration 2700 / 7000: loss 5.823780\n",
      "iteration 2800 / 7000: loss 5.243440\n",
      "iteration 2900 / 7000: loss 4.758943\n",
      "iteration 3000 / 7000: loss 4.354100\n",
      "iteration 3100 / 7000: loss 4.015870\n",
      "iteration 3200 / 7000: loss 3.733668\n",
      "iteration 3300 / 7000: loss 3.497658\n",
      "iteration 3400 / 7000: loss 3.300858\n",
      "iteration 3500 / 7000: loss 3.136298\n",
      "iteration 3600 / 7000: loss 2.998828\n",
      "iteration 3700 / 7000: loss 2.884096\n",
      "iteration 3800 / 7000: loss 2.788243\n",
      "iteration 3900 / 7000: loss 2.708172\n",
      "iteration 4000 / 7000: loss 2.641364\n",
      "iteration 4100 / 7000: loss 2.585477\n",
      "iteration 4200 / 7000: loss 2.538914\n",
      "iteration 4300 / 7000: loss 2.499951\n",
      "iteration 4400 / 7000: loss 2.467386\n",
      "iteration 4500 / 7000: loss 2.440209\n",
      "iteration 4600 / 7000: loss 2.417507\n",
      "iteration 4700 / 7000: loss 2.398557\n",
      "iteration 4800 / 7000: loss 2.382691\n",
      "iteration 4900 / 7000: loss 2.369525\n",
      "iteration 5000 / 7000: loss 2.358470\n",
      "iteration 5100 / 7000: loss 2.349282\n",
      "iteration 5200 / 7000: loss 2.341560\n",
      "iteration 5300 / 7000: loss 2.335135\n",
      "iteration 5400 / 7000: loss 2.329765\n",
      "iteration 5500 / 7000: loss 2.325288\n",
      "iteration 5600 / 7000: loss 2.321537\n",
      "iteration 5700 / 7000: loss 2.318385\n",
      "iteration 5800 / 7000: loss 2.315785\n",
      "iteration 5900 / 7000: loss 2.313614\n",
      "iteration 6000 / 7000: loss 2.311788\n",
      "iteration 6100 / 7000: loss 2.310245\n",
      "iteration 6200 / 7000: loss 2.308957\n",
      "iteration 6300 / 7000: loss 2.307930\n",
      "iteration 6400 / 7000: loss 2.307028\n",
      "iteration 6500 / 7000: loss 2.306297\n",
      "iteration 6600 / 7000: loss 2.305687\n",
      "iteration 6700 / 7000: loss 2.305114\n",
      "iteration 6800 / 7000: loss 2.304707\n",
      "iteration 6900 / 7000: loss 2.304381\n",
      "new acc  0.255\n",
      "lr, rs 3e-08 15000.0\n",
      "iteration 0 / 7000: loss 32.883587\n",
      "iteration 100 / 7000: loss 27.333459\n",
      "iteration 200 / 7000: loss 22.796087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 7000: loss 19.080694\n",
      "iteration 400 / 7000: loss 16.035961\n",
      "iteration 500 / 7000: loss 13.546236\n",
      "iteration 600 / 7000: loss 11.504547\n",
      "iteration 700 / 7000: loss 9.837661\n",
      "iteration 800 / 7000: loss 8.469689\n",
      "iteration 900 / 7000: loss 7.351947\n",
      "iteration 1000 / 7000: loss 6.435068\n",
      "iteration 1100 / 7000: loss 5.685805\n",
      "iteration 1200 / 7000: loss 5.073099\n",
      "iteration 1300 / 7000: loss 4.570476\n",
      "iteration 1400 / 7000: loss 4.159050\n",
      "iteration 1500 / 7000: loss 3.821905\n",
      "iteration 1600 / 7000: loss 3.546118\n",
      "iteration 1700 / 7000: loss 3.320637\n",
      "iteration 1800 / 7000: loss 3.135905\n",
      "iteration 1900 / 7000: loss 2.984879\n",
      "iteration 2000 / 7000: loss 2.860642\n",
      "iteration 2100 / 7000: loss 2.759364\n",
      "iteration 2200 / 7000: loss 2.675830\n",
      "iteration 2300 / 7000: loss 2.608206\n",
      "iteration 2400 / 7000: loss 2.552947\n",
      "iteration 2500 / 7000: loss 2.507205\n",
      "iteration 2600 / 7000: loss 2.470279\n",
      "iteration 2700 / 7000: loss 2.439028\n",
      "iteration 2800 / 7000: loss 2.414577\n",
      "iteration 2900 / 7000: loss 2.394379\n",
      "iteration 3000 / 7000: loss 2.377393\n",
      "iteration 3100 / 7000: loss 2.363334\n",
      "iteration 3200 / 7000: loss 2.351876\n",
      "iteration 3300 / 7000: loss 2.343138\n",
      "iteration 3400 / 7000: loss 2.335528\n",
      "iteration 3500 / 7000: loss 2.329623\n",
      "iteration 3600 / 7000: loss 2.324279\n",
      "iteration 3700 / 7000: loss 2.320154\n",
      "iteration 3800 / 7000: loss 2.316914\n",
      "iteration 3900 / 7000: loss 2.314373\n",
      "iteration 4000 / 7000: loss 2.312005\n",
      "iteration 4100 / 7000: loss 2.310356\n",
      "iteration 4200 / 7000: loss 2.308785\n",
      "iteration 4300 / 7000: loss 2.307487\n",
      "iteration 4400 / 7000: loss 2.306335\n",
      "iteration 4500 / 7000: loss 2.305520\n",
      "iteration 4600 / 7000: loss 2.304768\n",
      "iteration 4700 / 7000: loss 2.304135\n",
      "iteration 4800 / 7000: loss 2.303576\n",
      "iteration 4900 / 7000: loss 2.303446\n",
      "iteration 5000 / 7000: loss 2.303272\n",
      "iteration 5100 / 7000: loss 2.302666\n",
      "iteration 5200 / 7000: loss 2.302713\n",
      "iteration 5300 / 7000: loss 2.302636\n",
      "iteration 5400 / 7000: loss 2.302454\n",
      "iteration 5500 / 7000: loss 2.302492\n",
      "iteration 5600 / 7000: loss 2.301947\n",
      "iteration 5700 / 7000: loss 2.302130\n",
      "iteration 5800 / 7000: loss 2.302069\n",
      "iteration 5900 / 7000: loss 2.301883\n",
      "iteration 6000 / 7000: loss 2.302025\n",
      "iteration 6100 / 7000: loss 2.301987\n",
      "iteration 6200 / 7000: loss 2.301438\n",
      "iteration 6300 / 7000: loss 2.301326\n",
      "iteration 6400 / 7000: loss 2.301740\n",
      "iteration 6500 / 7000: loss 2.301716\n",
      "iteration 6600 / 7000: loss 2.301743\n",
      "iteration 6700 / 7000: loss 2.301943\n",
      "iteration 6800 / 7000: loss 2.301768\n",
      "iteration 6900 / 7000: loss 2.301808\n",
      "new acc  0.261\n",
      "lr, rs 5e-07 1000.0\n",
      "iteration 0 / 7000: loss 64.289247\n",
      "iteration 100 / 7000: loss 43.837600\n",
      "iteration 200 / 7000: loss 30.133001\n",
      "iteration 300 / 7000: loss 20.950485\n",
      "iteration 400 / 7000: loss 14.797288\n",
      "iteration 500 / 7000: loss 10.675416\n",
      "iteration 600 / 7000: loss 7.912045\n",
      "iteration 700 / 7000: loss 6.061505\n",
      "iteration 800 / 7000: loss 4.820468\n",
      "iteration 900 / 7000: loss 3.990008\n",
      "iteration 1000 / 7000: loss 3.432885\n",
      "iteration 1100 / 7000: loss 3.060029\n",
      "iteration 1200 / 7000: loss 2.809625\n",
      "iteration 1300 / 7000: loss 2.642323\n",
      "iteration 1400 / 7000: loss 2.529896\n",
      "iteration 1500 / 7000: loss 2.454806\n",
      "iteration 1600 / 7000: loss 2.404361\n",
      "iteration 1700 / 7000: loss 2.370808\n",
      "iteration 1800 / 7000: loss 2.348130\n",
      "iteration 1900 / 7000: loss 2.332974\n",
      "iteration 2000 / 7000: loss 2.322989\n",
      "iteration 2100 / 7000: loss 2.316072\n",
      "iteration 2200 / 7000: loss 2.311467\n",
      "iteration 2300 / 7000: loss 2.308324\n",
      "iteration 2400 / 7000: loss 2.306512\n",
      "iteration 2500 / 7000: loss 2.304770\n",
      "iteration 2600 / 7000: loss 2.304056\n",
      "iteration 2700 / 7000: loss 2.303210\n",
      "iteration 2800 / 7000: loss 2.303072\n",
      "iteration 2900 / 7000: loss 2.302613\n",
      "iteration 3000 / 7000: loss 2.302569\n",
      "iteration 3100 / 7000: loss 2.302286\n",
      "iteration 3200 / 7000: loss 2.302343\n",
      "iteration 3300 / 7000: loss 2.302141\n",
      "iteration 3400 / 7000: loss 2.302108\n",
      "iteration 3500 / 7000: loss 2.302226\n",
      "iteration 3600 / 7000: loss 2.301962\n",
      "iteration 3700 / 7000: loss 2.302274\n",
      "iteration 3800 / 7000: loss 2.302012\n",
      "iteration 3900 / 7000: loss 2.302172\n",
      "iteration 4000 / 7000: loss 2.302198\n",
      "iteration 4100 / 7000: loss 2.302402\n",
      "iteration 4200 / 7000: loss 2.302114\n",
      "iteration 4300 / 7000: loss 2.302113\n",
      "iteration 4400 / 7000: loss 2.301987\n",
      "iteration 4500 / 7000: loss 2.302161\n",
      "iteration 4600 / 7000: loss 2.302091\n",
      "iteration 4700 / 7000: loss 2.302087\n",
      "iteration 4800 / 7000: loss 2.302039\n",
      "iteration 4900 / 7000: loss 2.302196\n",
      "iteration 5000 / 7000: loss 2.301998\n",
      "iteration 5100 / 7000: loss 2.302009\n",
      "iteration 5200 / 7000: loss 2.302064\n",
      "iteration 5300 / 7000: loss 2.302354\n",
      "iteration 5400 / 7000: loss 2.302230\n",
      "iteration 5500 / 7000: loss 2.302038\n",
      "iteration 5600 / 7000: loss 2.302002\n",
      "iteration 5700 / 7000: loss 2.302197\n",
      "iteration 5800 / 7000: loss 2.302169\n",
      "iteration 5900 / 7000: loss 2.302099\n",
      "iteration 6000 / 7000: loss 2.302270\n",
      "iteration 6100 / 7000: loss 2.302087\n",
      "iteration 6200 / 7000: loss 2.302126\n",
      "iteration 6300 / 7000: loss 2.302182\n",
      "iteration 6400 / 7000: loss 2.302190\n",
      "iteration 6500 / 7000: loss 2.302093\n",
      "iteration 6600 / 7000: loss 2.302062\n",
      "iteration 6700 / 7000: loss 2.301929\n",
      "iteration 6800 / 7000: loss 2.301806\n",
      "iteration 6900 / 7000: loss 2.302244\n",
      "new acc  0.262\n",
      "lr, rs 5e-07 2000.0\n",
      "iteration 0 / 7000: loss 463.961383\n",
      "iteration 100 / 7000: loss 24.770439\n",
      "iteration 200 / 7000: loss 3.396062\n",
      "iteration 300 / 7000: loss 2.355719\n",
      "iteration 400 / 7000: loss 2.305122\n",
      "iteration 500 / 7000: loss 2.302660\n",
      "iteration 600 / 7000: loss 2.302530\n",
      "iteration 700 / 7000: loss 2.302533\n",
      "iteration 800 / 7000: loss 2.302522\n",
      "iteration 900 / 7000: loss 2.302539\n",
      "iteration 1000 / 7000: loss 2.302539\n",
      "iteration 1100 / 7000: loss 2.302534\n",
      "iteration 1200 / 7000: loss 2.302531\n",
      "iteration 1300 / 7000: loss 2.302519\n",
      "iteration 1400 / 7000: loss 2.302539\n",
      "iteration 1500 / 7000: loss 2.302514\n",
      "iteration 1600 / 7000: loss 2.302530\n",
      "iteration 1700 / 7000: loss 2.302513\n",
      "iteration 1800 / 7000: loss 2.302516\n",
      "iteration 1900 / 7000: loss 2.302508\n",
      "iteration 2000 / 7000: loss 2.302511\n",
      "iteration 2100 / 7000: loss 2.302495\n",
      "iteration 2200 / 7000: loss 2.302539\n",
      "iteration 2300 / 7000: loss 2.302536\n",
      "iteration 2400 / 7000: loss 2.302512\n",
      "iteration 2500 / 7000: loss 2.302521\n",
      "iteration 2600 / 7000: loss 2.302524\n",
      "iteration 2700 / 7000: loss 2.302548\n",
      "iteration 2800 / 7000: loss 2.302505\n",
      "iteration 2900 / 7000: loss 2.302504\n",
      "iteration 3000 / 7000: loss 2.302526\n",
      "iteration 3100 / 7000: loss 2.302525\n",
      "iteration 3200 / 7000: loss 2.302521\n",
      "iteration 3300 / 7000: loss 2.302535\n",
      "iteration 3400 / 7000: loss 2.302535\n",
      "iteration 3500 / 7000: loss 2.302521\n",
      "iteration 3600 / 7000: loss 2.302529\n",
      "iteration 3700 / 7000: loss 2.302503\n",
      "iteration 3800 / 7000: loss 2.302533\n",
      "iteration 3900 / 7000: loss 2.302551\n",
      "iteration 4000 / 7000: loss 2.302554\n",
      "iteration 4100 / 7000: loss 2.302541\n",
      "iteration 4200 / 7000: loss 2.302543\n",
      "iteration 4300 / 7000: loss 2.302520\n",
      "iteration 4400 / 7000: loss 2.302521\n",
      "iteration 4500 / 7000: loss 2.302511\n",
      "iteration 4600 / 7000: loss 2.302545\n",
      "iteration 4700 / 7000: loss 2.302513\n",
      "iteration 4800 / 7000: loss 2.302503\n",
      "iteration 4900 / 7000: loss 2.302539\n",
      "iteration 5000 / 7000: loss 2.302512\n",
      "iteration 5100 / 7000: loss 2.302536\n",
      "iteration 5200 / 7000: loss 2.302504\n",
      "iteration 5300 / 7000: loss 2.302530\n",
      "iteration 5400 / 7000: loss 2.302531\n",
      "iteration 5500 / 7000: loss 2.302556\n",
      "iteration 5600 / 7000: loss 2.302526\n",
      "iteration 5700 / 7000: loss 2.302522\n",
      "iteration 5800 / 7000: loss 2.302519\n",
      "iteration 5900 / 7000: loss 2.302539\n",
      "iteration 6000 / 7000: loss 2.302534\n",
      "iteration 6100 / 7000: loss 2.302523\n",
      "iteration 6200 / 7000: loss 2.302547\n",
      "iteration 6300 / 7000: loss 2.302514\n",
      "iteration 6400 / 7000: loss 2.302520\n",
      "iteration 6500 / 7000: loss 2.302529\n",
      "iteration 6600 / 7000: loss 2.302533\n",
      "iteration 6700 / 7000: loss 2.302510\n",
      "iteration 6800 / 7000: loss 2.302509\n",
      "iteration 6900 / 7000: loss 2.302515\n",
      "new acc  0.253\n",
      "lr, rs 5e-07 15000.0\n",
      "iteration 0 / 7000: loss 33.635390\n",
      "iteration 100 / 7000: loss 23.295715\n",
      "iteration 200 / 7000: loss 16.366567\n",
      "iteration 300 / 7000: loss 11.726557\n",
      "iteration 400 / 7000: loss 8.618796\n",
      "iteration 500 / 7000: loss 6.532938\n",
      "iteration 600 / 7000: loss 5.137513\n",
      "iteration 700 / 7000: loss 4.201836\n",
      "iteration 800 / 7000: loss 3.574797\n",
      "iteration 900 / 7000: loss 3.154562\n",
      "iteration 1000 / 7000: loss 2.873450\n",
      "iteration 1100 / 7000: loss 2.684331\n",
      "iteration 1200 / 7000: loss 2.558748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1300 / 7000: loss 2.473882\n",
      "iteration 1400 / 7000: loss 2.416985\n",
      "iteration 1500 / 7000: loss 2.378794\n",
      "iteration 1600 / 7000: loss 2.353607\n",
      "iteration 1700 / 7000: loss 2.336173\n",
      "iteration 1800 / 7000: loss 2.324787\n",
      "iteration 1900 / 7000: loss 2.317403\n",
      "iteration 2000 / 7000: loss 2.312405\n",
      "iteration 2100 / 7000: loss 2.308380\n",
      "iteration 2200 / 7000: loss 2.306613\n",
      "iteration 2300 / 7000: loss 2.304830\n",
      "iteration 2400 / 7000: loss 2.303987\n",
      "iteration 2500 / 7000: loss 2.303096\n",
      "iteration 2600 / 7000: loss 2.302711\n",
      "iteration 2700 / 7000: loss 2.302020\n",
      "iteration 2800 / 7000: loss 2.302267\n",
      "iteration 2900 / 7000: loss 2.302228\n",
      "iteration 3000 / 7000: loss 2.302002\n",
      "iteration 3100 / 7000: loss 2.301585\n",
      "iteration 3200 / 7000: loss 2.301586\n",
      "iteration 3300 / 7000: loss 2.301784\n",
      "iteration 3400 / 7000: loss 2.301712\n",
      "iteration 3500 / 7000: loss 2.301527\n",
      "iteration 3600 / 7000: loss 2.301408\n",
      "iteration 3700 / 7000: loss 2.302028\n",
      "iteration 3800 / 7000: loss 2.301414\n",
      "iteration 3900 / 7000: loss 2.301226\n",
      "iteration 4000 / 7000: loss 2.301451\n",
      "iteration 4100 / 7000: loss 2.301929\n",
      "iteration 4200 / 7000: loss 2.302264\n",
      "iteration 4300 / 7000: loss 2.301367\n",
      "iteration 4400 / 7000: loss 2.301490\n",
      "iteration 4500 / 7000: loss 2.302067\n",
      "iteration 4600 / 7000: loss 2.301867\n",
      "iteration 4700 / 7000: loss 2.301771\n",
      "iteration 4800 / 7000: loss 2.302078\n",
      "iteration 4900 / 7000: loss 2.301779\n",
      "iteration 5000 / 7000: loss 2.301957\n",
      "iteration 5100 / 7000: loss 2.301930\n",
      "iteration 5200 / 7000: loss 2.301915\n",
      "iteration 5300 / 7000: loss 2.301648\n",
      "iteration 5400 / 7000: loss 2.301838\n",
      "iteration 5500 / 7000: loss 2.301866\n",
      "iteration 5600 / 7000: loss 2.301717\n",
      "iteration 5700 / 7000: loss 2.301948\n",
      "iteration 5800 / 7000: loss 2.301476\n",
      "iteration 5900 / 7000: loss 2.301736\n",
      "iteration 6000 / 7000: loss 2.301906\n",
      "iteration 6100 / 7000: loss 2.301740\n",
      "iteration 6200 / 7000: loss 2.301651\n",
      "iteration 6300 / 7000: loss 2.301469\n",
      "iteration 6400 / 7000: loss 2.301255\n",
      "iteration 6500 / 7000: loss 2.301736\n",
      "iteration 6600 / 7000: loss 2.301438\n",
      "iteration 6700 / 7000: loss 2.301173\n",
      "iteration 6800 / 7000: loss 2.302199\n",
      "iteration 6900 / 7000: loss 2.301314\n",
      "new acc  0.254\n",
      "lr, rs 1e-06 1000.0\n",
      "iteration 0 / 7000: loss 63.913780\n",
      "iteration 100 / 7000: loss 29.943417\n",
      "iteration 200 / 7000: loss 14.700208\n",
      "iteration 300 / 7000: loss 7.864911\n",
      "iteration 400 / 7000: loss 4.797821\n",
      "iteration 500 / 7000: loss 3.421953\n",
      "iteration 600 / 7000: loss 2.804377\n",
      "iteration 700 / 7000: loss 2.527492\n",
      "iteration 800 / 7000: loss 2.403087\n",
      "iteration 900 / 7000: loss 2.347567\n",
      "iteration 1000 / 7000: loss 2.322745\n",
      "iteration 1100 / 7000: loss 2.311137\n",
      "iteration 1200 / 7000: loss 2.306222\n",
      "iteration 1300 / 7000: loss 2.304015\n",
      "iteration 1400 / 7000: loss 2.302944\n",
      "iteration 1500 / 7000: loss 2.302436\n",
      "iteration 1600 / 7000: loss 2.302312\n",
      "iteration 1700 / 7000: loss 2.302100\n",
      "iteration 1800 / 7000: loss 2.302131\n",
      "iteration 1900 / 7000: loss 2.302295\n",
      "iteration 2000 / 7000: loss 2.302088\n",
      "iteration 2100 / 7000: loss 2.302136\n",
      "iteration 2200 / 7000: loss 2.302202\n",
      "iteration 2300 / 7000: loss 2.302121\n",
      "iteration 2400 / 7000: loss 2.302182\n",
      "iteration 2500 / 7000: loss 2.302101\n",
      "iteration 2600 / 7000: loss 2.302096\n",
      "iteration 2700 / 7000: loss 2.302130\n",
      "iteration 2800 / 7000: loss 2.302188\n",
      "iteration 2900 / 7000: loss 2.302075\n",
      "iteration 3000 / 7000: loss 2.302131\n",
      "iteration 3100 / 7000: loss 2.302210\n",
      "iteration 3200 / 7000: loss 2.301920\n",
      "iteration 3300 / 7000: loss 2.302101\n",
      "iteration 3400 / 7000: loss 2.302165\n",
      "iteration 3500 / 7000: loss 2.302132\n",
      "iteration 3600 / 7000: loss 2.302157\n",
      "iteration 3700 / 7000: loss 2.301961\n",
      "iteration 3800 / 7000: loss 2.302040\n",
      "iteration 3900 / 7000: loss 2.302291\n",
      "iteration 4000 / 7000: loss 2.302165\n",
      "iteration 4100 / 7000: loss 2.302070\n",
      "iteration 4200 / 7000: loss 2.302002\n",
      "iteration 4300 / 7000: loss 2.302154\n",
      "iteration 4400 / 7000: loss 2.302164\n",
      "iteration 4500 / 7000: loss 2.302085\n",
      "iteration 4600 / 7000: loss 2.301981\n",
      "iteration 4700 / 7000: loss 2.302054\n",
      "iteration 4800 / 7000: loss 2.302005\n",
      "iteration 4900 / 7000: loss 2.302256\n",
      "iteration 5000 / 7000: loss 2.302106\n",
      "iteration 5100 / 7000: loss 2.301993\n",
      "iteration 5200 / 7000: loss 2.302072\n",
      "iteration 5300 / 7000: loss 2.302094\n",
      "iteration 5400 / 7000: loss 2.302204\n",
      "iteration 5500 / 7000: loss 2.302337\n",
      "iteration 5600 / 7000: loss 2.302170\n",
      "iteration 5700 / 7000: loss 2.302111\n",
      "iteration 5800 / 7000: loss 2.302078\n",
      "iteration 5900 / 7000: loss 2.302020\n",
      "iteration 6000 / 7000: loss 2.302045\n",
      "iteration 6100 / 7000: loss 2.302194\n",
      "iteration 6200 / 7000: loss 2.302120\n",
      "iteration 6300 / 7000: loss 2.302039\n",
      "iteration 6400 / 7000: loss 2.302184\n",
      "iteration 6500 / 7000: loss 2.302122\n",
      "iteration 6600 / 7000: loss 2.302129\n",
      "iteration 6700 / 7000: loss 2.302256\n",
      "iteration 6800 / 7000: loss 2.302102\n",
      "iteration 6900 / 7000: loss 2.302255\n",
      "new acc  0.262\n",
      "lr, rs 1e-06 2000.0\n",
      "iteration 0 / 7000: loss 464.805559\n",
      "iteration 100 / 7000: loss 3.348311\n",
      "iteration 200 / 7000: loss 2.304899\n",
      "iteration 300 / 7000: loss 2.302537\n",
      "iteration 400 / 7000: loss 2.302540\n",
      "iteration 500 / 7000: loss 2.302553\n",
      "iteration 600 / 7000: loss 2.302531\n",
      "iteration 700 / 7000: loss 2.302534\n",
      "iteration 800 / 7000: loss 2.302529\n",
      "iteration 900 / 7000: loss 2.302545\n",
      "iteration 1000 / 7000: loss 2.302523\n",
      "iteration 1100 / 7000: loss 2.302549\n",
      "iteration 1200 / 7000: loss 2.302522\n",
      "iteration 1300 / 7000: loss 2.302528\n",
      "iteration 1400 / 7000: loss 2.302538\n",
      "iteration 1500 / 7000: loss 2.302524\n",
      "iteration 1600 / 7000: loss 2.302539\n",
      "iteration 1700 / 7000: loss 2.302533\n",
      "iteration 1800 / 7000: loss 2.302524\n",
      "iteration 1900 / 7000: loss 2.302522\n",
      "iteration 2000 / 7000: loss 2.302538\n",
      "iteration 2100 / 7000: loss 2.302546\n",
      "iteration 2200 / 7000: loss 2.302525\n",
      "iteration 2300 / 7000: loss 2.302510\n",
      "iteration 2400 / 7000: loss 2.302514\n",
      "iteration 2500 / 7000: loss 2.302563\n",
      "iteration 2600 / 7000: loss 2.302531\n",
      "iteration 2700 / 7000: loss 2.302534\n",
      "iteration 2800 / 7000: loss 2.302522\n",
      "iteration 2900 / 7000: loss 2.302528\n",
      "iteration 3000 / 7000: loss 2.302540\n",
      "iteration 3100 / 7000: loss 2.302541\n",
      "iteration 3200 / 7000: loss 2.302515\n",
      "iteration 3300 / 7000: loss 2.302548\n",
      "iteration 3400 / 7000: loss 2.302506\n",
      "iteration 3500 / 7000: loss 2.302512\n",
      "iteration 3600 / 7000: loss 2.302538\n",
      "iteration 3700 / 7000: loss 2.302515\n",
      "iteration 3800 / 7000: loss 2.302527\n",
      "iteration 3900 / 7000: loss 2.302535\n",
      "iteration 4000 / 7000: loss 2.302537\n",
      "iteration 4100 / 7000: loss 2.302532\n",
      "iteration 4200 / 7000: loss 2.302534\n",
      "iteration 4300 / 7000: loss 2.302519\n",
      "iteration 4400 / 7000: loss 2.302522\n",
      "iteration 4500 / 7000: loss 2.302513\n",
      "iteration 4600 / 7000: loss 2.302522\n",
      "iteration 4700 / 7000: loss 2.302532\n",
      "iteration 4800 / 7000: loss 2.302515\n",
      "iteration 4900 / 7000: loss 2.302524\n",
      "iteration 5000 / 7000: loss 2.302531\n",
      "iteration 5100 / 7000: loss 2.302527\n",
      "iteration 5200 / 7000: loss 2.302520\n",
      "iteration 5300 / 7000: loss 2.302546\n",
      "iteration 5400 / 7000: loss 2.302513\n",
      "iteration 5500 / 7000: loss 2.302528\n",
      "iteration 5600 / 7000: loss 2.302531\n",
      "iteration 5700 / 7000: loss 2.302560\n",
      "iteration 5800 / 7000: loss 2.302525\n",
      "iteration 5900 / 7000: loss 2.302545\n",
      "iteration 6000 / 7000: loss 2.302539\n",
      "iteration 6100 / 7000: loss 2.302526\n",
      "iteration 6200 / 7000: loss 2.302555\n",
      "iteration 6300 / 7000: loss 2.302520\n",
      "iteration 6400 / 7000: loss 2.302531\n",
      "iteration 6500 / 7000: loss 2.302524\n",
      "iteration 6600 / 7000: loss 2.302510\n",
      "iteration 6700 / 7000: loss 2.302512\n",
      "iteration 6800 / 7000: loss 2.302519\n",
      "iteration 6900 / 7000: loss 2.302521\n",
      "new acc  0.262\n",
      "lr, rs 1e-06 15000.0\n",
      "iteration 0 / 7000: loss 32.901629\n",
      "iteration 100 / 7000: loss 6.402484\n",
      "iteration 200 / 7000: loss 2.850673\n",
      "iteration 300 / 7000: loss 2.375156\n",
      "iteration 400 / 7000: loss 2.311873\n",
      "iteration 500 / 7000: loss 2.302945\n",
      "iteration 600 / 7000: loss 2.301537\n",
      "iteration 700 / 7000: loss 2.301710\n",
      "iteration 800 / 7000: loss 2.301908\n",
      "iteration 900 / 7000: loss 2.301885\n",
      "iteration 1000 / 7000: loss 2.301699\n",
      "iteration 1100 / 7000: loss 2.301528\n",
      "iteration 1200 / 7000: loss 2.301866\n",
      "iteration 1300 / 7000: loss 2.301509\n",
      "iteration 1400 / 7000: loss 2.301768\n",
      "iteration 1500 / 7000: loss 2.301798\n",
      "iteration 1600 / 7000: loss 2.301516\n",
      "iteration 1700 / 7000: loss 2.301956\n",
      "iteration 1800 / 7000: loss 2.301731\n",
      "iteration 1900 / 7000: loss 2.301580\n",
      "iteration 2000 / 7000: loss 2.301773\n",
      "iteration 2100 / 7000: loss 2.301895\n",
      "iteration 2200 / 7000: loss 2.301407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2300 / 7000: loss 2.301941\n",
      "iteration 2400 / 7000: loss 2.301563\n",
      "iteration 2500 / 7000: loss 2.301530\n",
      "iteration 2600 / 7000: loss 2.301778\n",
      "iteration 2700 / 7000: loss 2.301380\n",
      "iteration 2800 / 7000: loss 2.301603\n",
      "iteration 2900 / 7000: loss 2.301190\n",
      "iteration 3000 / 7000: loss 2.301425\n",
      "iteration 3100 / 7000: loss 2.301299\n",
      "iteration 3200 / 7000: loss 2.301579\n",
      "iteration 3300 / 7000: loss 2.301363\n",
      "iteration 3400 / 7000: loss 2.301353\n",
      "iteration 3500 / 7000: loss 2.301297\n",
      "iteration 3600 / 7000: loss 2.301652\n",
      "iteration 3700 / 7000: loss 2.301779\n",
      "iteration 3800 / 7000: loss 2.301746\n",
      "iteration 3900 / 7000: loss 2.301731\n",
      "iteration 4000 / 7000: loss 2.301651\n",
      "iteration 4100 / 7000: loss 2.301618\n",
      "iteration 4200 / 7000: loss 2.301822\n",
      "iteration 4300 / 7000: loss 2.301656\n",
      "iteration 4400 / 7000: loss 2.301860\n",
      "iteration 4500 / 7000: loss 2.301991\n",
      "iteration 4600 / 7000: loss 2.301719\n",
      "iteration 4700 / 7000: loss 2.301757\n",
      "iteration 4800 / 7000: loss 2.301554\n",
      "iteration 4900 / 7000: loss 2.301889\n",
      "iteration 5000 / 7000: loss 2.301843\n",
      "iteration 5100 / 7000: loss 2.301517\n",
      "iteration 5200 / 7000: loss 2.301578\n",
      "iteration 5300 / 7000: loss 2.301312\n",
      "iteration 5400 / 7000: loss 2.301307\n",
      "iteration 5500 / 7000: loss 2.301472\n",
      "iteration 5600 / 7000: loss 2.301594\n",
      "iteration 5700 / 7000: loss 2.301794\n",
      "iteration 5800 / 7000: loss 2.301581\n",
      "iteration 5900 / 7000: loss 2.301865\n",
      "iteration 6000 / 7000: loss 2.302003\n",
      "iteration 6100 / 7000: loss 2.301417\n",
      "iteration 6200 / 7000: loss 2.301951\n",
      "iteration 6300 / 7000: loss 2.301874\n",
      "iteration 6400 / 7000: loss 2.301567\n",
      "iteration 6500 / 7000: loss 2.301734\n",
      "iteration 6600 / 7000: loss 2.301715\n",
      "iteration 6700 / 7000: loss 2.301573\n",
      "iteration 6800 / 7000: loss 2.301675\n",
      "iteration 6900 / 7000: loss 2.301666\n",
      "new acc  0.26\n",
      "lr, rs 5e-06 1000.0\n",
      "iteration 0 / 7000: loss 63.690999\n",
      "iteration 100 / 7000: loss 3.382038\n",
      "iteration 200 / 7000: loss 2.320932\n",
      "iteration 300 / 7000: loss 2.302519\n",
      "iteration 400 / 7000: loss 2.301997\n",
      "iteration 500 / 7000: loss 2.302160\n",
      "iteration 600 / 7000: loss 2.302192\n",
      "iteration 700 / 7000: loss 2.302273\n",
      "iteration 800 / 7000: loss 2.302152\n",
      "iteration 900 / 7000: loss 2.302071\n",
      "iteration 1000 / 7000: loss 2.302132\n",
      "iteration 1100 / 7000: loss 2.302232\n",
      "iteration 1200 / 7000: loss 2.302048\n",
      "iteration 1300 / 7000: loss 2.302037\n",
      "iteration 1400 / 7000: loss 2.302153\n",
      "iteration 1500 / 7000: loss 2.302097\n",
      "iteration 1600 / 7000: loss 2.302047\n",
      "iteration 1700 / 7000: loss 2.302127\n",
      "iteration 1800 / 7000: loss 2.302097\n",
      "iteration 1900 / 7000: loss 2.302213\n",
      "iteration 2000 / 7000: loss 2.302096\n",
      "iteration 2100 / 7000: loss 2.302157\n",
      "iteration 2200 / 7000: loss 2.301962\n",
      "iteration 2300 / 7000: loss 2.302126\n",
      "iteration 2400 / 7000: loss 2.302078\n",
      "iteration 2500 / 7000: loss 2.302276\n",
      "iteration 2600 / 7000: loss 2.302198\n",
      "iteration 2700 / 7000: loss 2.302138\n",
      "iteration 2800 / 7000: loss 2.302070\n",
      "iteration 2900 / 7000: loss 2.302256\n",
      "iteration 3000 / 7000: loss 2.302196\n",
      "iteration 3100 / 7000: loss 2.302258\n",
      "iteration 3200 / 7000: loss 2.302092\n",
      "iteration 3300 / 7000: loss 2.302167\n",
      "iteration 3400 / 7000: loss 2.302040\n",
      "iteration 3500 / 7000: loss 2.302223\n",
      "iteration 3600 / 7000: loss 2.302111\n",
      "iteration 3700 / 7000: loss 2.302139\n",
      "iteration 3800 / 7000: loss 2.301966\n",
      "iteration 3900 / 7000: loss 2.302082\n",
      "iteration 4000 / 7000: loss 2.302189\n",
      "iteration 4100 / 7000: loss 2.301959\n",
      "iteration 4200 / 7000: loss 2.302241\n",
      "iteration 4300 / 7000: loss 2.302041\n",
      "iteration 4400 / 7000: loss 2.302182\n",
      "iteration 4500 / 7000: loss 2.302083\n",
      "iteration 4600 / 7000: loss 2.301995\n",
      "iteration 4700 / 7000: loss 2.302218\n",
      "iteration 4800 / 7000: loss 2.302259\n",
      "iteration 4900 / 7000: loss 2.302151\n",
      "iteration 5000 / 7000: loss 2.302139\n",
      "iteration 5100 / 7000: loss 2.302118\n",
      "iteration 5200 / 7000: loss 2.302045\n",
      "iteration 5300 / 7000: loss 2.302112\n",
      "iteration 5400 / 7000: loss 2.302296\n",
      "iteration 5500 / 7000: loss 2.302169\n",
      "iteration 5600 / 7000: loss 2.302123\n",
      "iteration 5700 / 7000: loss 2.302008\n",
      "iteration 5800 / 7000: loss 2.302111\n",
      "iteration 5900 / 7000: loss 2.302380\n",
      "iteration 6000 / 7000: loss 2.302268\n",
      "iteration 6100 / 7000: loss 2.302277\n",
      "iteration 6200 / 7000: loss 2.301950\n",
      "iteration 6300 / 7000: loss 2.302180\n",
      "iteration 6400 / 7000: loss 2.302129\n",
      "iteration 6500 / 7000: loss 2.302063\n",
      "iteration 6600 / 7000: loss 2.302228\n",
      "iteration 6700 / 7000: loss 2.302237\n",
      "iteration 6800 / 7000: loss 2.302137\n",
      "iteration 6900 / 7000: loss 2.302183\n",
      "new acc  0.26\n",
      "lr, rs 5e-06 2000.0\n",
      "iteration 0 / 7000: loss 465.827612\n",
      "iteration 100 / 7000: loss 2.302530\n",
      "iteration 200 / 7000: loss 2.302525\n",
      "iteration 300 / 7000: loss 2.302525\n",
      "iteration 400 / 7000: loss 2.302497\n",
      "iteration 500 / 7000: loss 2.302518\n",
      "iteration 600 / 7000: loss 2.302533\n",
      "iteration 700 / 7000: loss 2.302549\n",
      "iteration 800 / 7000: loss 2.302533\n",
      "iteration 900 / 7000: loss 2.302542\n",
      "iteration 1000 / 7000: loss 2.302543\n",
      "iteration 1100 / 7000: loss 2.302523\n",
      "iteration 1200 / 7000: loss 2.302539\n",
      "iteration 1300 / 7000: loss 2.302472\n",
      "iteration 1400 / 7000: loss 2.302532\n",
      "iteration 1500 / 7000: loss 2.302531\n",
      "iteration 1600 / 7000: loss 2.302531\n",
      "iteration 1700 / 7000: loss 2.302511\n",
      "iteration 1800 / 7000: loss 2.302558\n",
      "iteration 1900 / 7000: loss 2.302531\n",
      "iteration 2000 / 7000: loss 2.302518\n",
      "iteration 2100 / 7000: loss 2.302518\n",
      "iteration 2200 / 7000: loss 2.302515\n",
      "iteration 2300 / 7000: loss 2.302482\n",
      "iteration 2400 / 7000: loss 2.302523\n",
      "iteration 2500 / 7000: loss 2.302534\n",
      "iteration 2600 / 7000: loss 2.302543\n",
      "iteration 2700 / 7000: loss 2.302519\n",
      "iteration 2800 / 7000: loss 2.302525\n",
      "iteration 2900 / 7000: loss 2.302539\n",
      "iteration 3000 / 7000: loss 2.302527\n",
      "iteration 3100 / 7000: loss 2.302550\n",
      "iteration 3200 / 7000: loss 2.302532\n",
      "iteration 3300 / 7000: loss 2.302526\n",
      "iteration 3400 / 7000: loss 2.302520\n",
      "iteration 3500 / 7000: loss 2.302535\n",
      "iteration 3600 / 7000: loss 2.302542\n",
      "iteration 3700 / 7000: loss 2.302526\n",
      "iteration 3800 / 7000: loss 2.302523\n",
      "iteration 3900 / 7000: loss 2.302519\n",
      "iteration 4000 / 7000: loss 2.302517\n",
      "iteration 4100 / 7000: loss 2.302524\n",
      "iteration 4200 / 7000: loss 2.302520\n",
      "iteration 4300 / 7000: loss 2.302530\n",
      "iteration 4400 / 7000: loss 2.302531\n",
      "iteration 4500 / 7000: loss 2.302528\n",
      "iteration 4600 / 7000: loss 2.302526\n",
      "iteration 4700 / 7000: loss 2.302538\n",
      "iteration 4800 / 7000: loss 2.302521\n",
      "iteration 4900 / 7000: loss 2.302525\n",
      "iteration 5000 / 7000: loss 2.302526\n",
      "iteration 5100 / 7000: loss 2.302554\n",
      "iteration 5200 / 7000: loss 2.302524\n",
      "iteration 5300 / 7000: loss 2.302513\n",
      "iteration 5400 / 7000: loss 2.302485\n",
      "iteration 5500 / 7000: loss 2.302532\n",
      "iteration 5600 / 7000: loss 2.302525\n",
      "iteration 5700 / 7000: loss 2.302517\n",
      "iteration 5800 / 7000: loss 2.302537\n",
      "iteration 5900 / 7000: loss 2.302541\n",
      "iteration 6000 / 7000: loss 2.302524\n",
      "iteration 6100 / 7000: loss 2.302515\n",
      "iteration 6200 / 7000: loss 2.302559\n",
      "iteration 6300 / 7000: loss 2.302540\n",
      "iteration 6400 / 7000: loss 2.302507\n",
      "iteration 6500 / 7000: loss 2.302522\n",
      "iteration 6600 / 7000: loss 2.302517\n",
      "iteration 6700 / 7000: loss 2.302525\n",
      "iteration 6800 / 7000: loss 2.302503\n",
      "iteration 6900 / 7000: loss 2.302533\n",
      "new acc  0.252\n",
      "lr, rs 5e-06 15000.0\n",
      "iteration 0 / 7000: loss 33.081329\n",
      "iteration 100 / 7000: loss 2.842912\n",
      "iteration 200 / 7000: loss 2.311417\n",
      "iteration 300 / 7000: loss 2.301784\n",
      "iteration 400 / 7000: loss 2.301898\n",
      "iteration 500 / 7000: loss 2.302020\n",
      "iteration 600 / 7000: loss 2.301869\n",
      "iteration 700 / 7000: loss 2.301982\n",
      "iteration 800 / 7000: loss 2.302243\n",
      "iteration 900 / 7000: loss 2.301768\n",
      "iteration 1000 / 7000: loss 2.301940\n",
      "iteration 1100 / 7000: loss 2.301796\n",
      "iteration 1200 / 7000: loss 2.301529\n",
      "iteration 1300 / 7000: loss 2.301589\n",
      "iteration 1400 / 7000: loss 2.301803\n",
      "iteration 1500 / 7000: loss 2.301904\n",
      "iteration 1600 / 7000: loss 2.302093\n",
      "iteration 1700 / 7000: loss 2.301799\n",
      "iteration 1800 / 7000: loss 2.301859\n",
      "iteration 1900 / 7000: loss 2.301399\n",
      "iteration 2000 / 7000: loss 2.301734\n",
      "iteration 2100 / 7000: loss 2.301856\n",
      "iteration 2200 / 7000: loss 2.301767\n",
      "iteration 2300 / 7000: loss 2.301901\n",
      "iteration 2400 / 7000: loss 2.302007\n",
      "iteration 2500 / 7000: loss 2.301753\n",
      "iteration 2600 / 7000: loss 2.301522\n",
      "iteration 2700 / 7000: loss 2.301667\n",
      "iteration 2800 / 7000: loss 2.301577\n",
      "iteration 2900 / 7000: loss 2.301885\n",
      "iteration 3000 / 7000: loss 2.301618\n",
      "iteration 3100 / 7000: loss 2.301469\n",
      "iteration 3200 / 7000: loss 2.301680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3300 / 7000: loss 2.301703\n",
      "iteration 3400 / 7000: loss 2.301570\n",
      "iteration 3500 / 7000: loss 2.301731\n",
      "iteration 3600 / 7000: loss 2.301785\n",
      "iteration 3700 / 7000: loss 2.301733\n",
      "iteration 3800 / 7000: loss 2.301853\n",
      "iteration 3900 / 7000: loss 2.301445\n",
      "iteration 4000 / 7000: loss 2.301490\n",
      "iteration 4100 / 7000: loss 2.301442\n",
      "iteration 4200 / 7000: loss 2.301360\n",
      "iteration 4300 / 7000: loss 2.301571\n",
      "iteration 4400 / 7000: loss 2.301549\n",
      "iteration 4500 / 7000: loss 2.301827\n",
      "iteration 4600 / 7000: loss 2.301926\n",
      "iteration 4700 / 7000: loss 2.301723\n",
      "iteration 4800 / 7000: loss 2.301453\n",
      "iteration 4900 / 7000: loss 2.301486\n",
      "iteration 5000 / 7000: loss 2.301876\n",
      "iteration 5100 / 7000: loss 2.301737\n",
      "iteration 5200 / 7000: loss 2.302063\n",
      "iteration 5300 / 7000: loss 2.301510\n",
      "iteration 5400 / 7000: loss 2.301744\n",
      "iteration 5500 / 7000: loss 2.301817\n",
      "iteration 5600 / 7000: loss 2.301485\n",
      "iteration 5700 / 7000: loss 2.301772\n",
      "iteration 5800 / 7000: loss 2.301749\n",
      "iteration 5900 / 7000: loss 2.301839\n",
      "iteration 6000 / 7000: loss 2.301540\n",
      "iteration 6100 / 7000: loss 2.301330\n",
      "iteration 6200 / 7000: loss 2.301919\n",
      "iteration 6300 / 7000: loss 2.301915\n",
      "iteration 6400 / 7000: loss 2.301928\n",
      "iteration 6500 / 7000: loss 2.301558\n",
      "iteration 6600 / 7000: loss 2.301735\n",
      "iteration 6700 / 7000: loss 2.301775\n",
      "iteration 6800 / 7000: loss 2.301410\n",
      "iteration 6900 / 7000: loss 2.301796\n",
      "new acc  0.257\n",
      "lr, rs 1e-05 1000.0\n",
      "iteration 0 / 7000: loss 63.215514\n",
      "iteration 100 / 7000: loss 2.319362\n",
      "iteration 200 / 7000: loss 2.302088\n",
      "iteration 300 / 7000: loss 2.302041\n",
      "iteration 400 / 7000: loss 2.302045\n",
      "iteration 500 / 7000: loss 2.302143\n",
      "iteration 600 / 7000: loss 2.302230\n",
      "iteration 700 / 7000: loss 2.302145\n",
      "iteration 800 / 7000: loss 2.302203\n",
      "iteration 900 / 7000: loss 2.302197\n",
      "iteration 1000 / 7000: loss 2.302177\n",
      "iteration 1100 / 7000: loss 2.302066\n",
      "iteration 1200 / 7000: loss 2.302288\n",
      "iteration 1300 / 7000: loss 2.302266\n",
      "iteration 1400 / 7000: loss 2.302207\n",
      "iteration 1500 / 7000: loss 2.302334\n",
      "iteration 1600 / 7000: loss 2.302168\n",
      "iteration 1700 / 7000: loss 2.302111\n",
      "iteration 1800 / 7000: loss 2.302219\n",
      "iteration 1900 / 7000: loss 2.302138\n",
      "iteration 2000 / 7000: loss 2.302002\n",
      "iteration 2100 / 7000: loss 2.302112\n",
      "iteration 2200 / 7000: loss 2.302252\n",
      "iteration 2300 / 7000: loss 2.302276\n",
      "iteration 2400 / 7000: loss 2.301997\n",
      "iteration 2500 / 7000: loss 2.302299\n",
      "iteration 2600 / 7000: loss 2.302023\n",
      "iteration 2700 / 7000: loss 2.302168\n",
      "iteration 2800 / 7000: loss 2.302124\n",
      "iteration 2900 / 7000: loss 2.302146\n",
      "iteration 3000 / 7000: loss 2.302137\n",
      "iteration 3100 / 7000: loss 2.302037\n",
      "iteration 3200 / 7000: loss 2.302423\n",
      "iteration 3300 / 7000: loss 2.302065\n",
      "iteration 3400 / 7000: loss 2.302244\n",
      "iteration 3500 / 7000: loss 2.302111\n",
      "iteration 3600 / 7000: loss 2.302197\n",
      "iteration 3700 / 7000: loss 2.302121\n",
      "iteration 3800 / 7000: loss 2.302141\n",
      "iteration 3900 / 7000: loss 2.302216\n",
      "iteration 4000 / 7000: loss 2.302380\n",
      "iteration 4100 / 7000: loss 2.301954\n",
      "iteration 4200 / 7000: loss 2.302078\n",
      "iteration 4300 / 7000: loss 2.302252\n",
      "iteration 4400 / 7000: loss 2.302186\n",
      "iteration 4500 / 7000: loss 2.301967\n",
      "iteration 4600 / 7000: loss 2.302094\n",
      "iteration 4700 / 7000: loss 2.302337\n",
      "iteration 4800 / 7000: loss 2.302185\n",
      "iteration 4900 / 7000: loss 2.302212\n",
      "iteration 5000 / 7000: loss 2.301973\n",
      "iteration 5100 / 7000: loss 2.302276\n",
      "iteration 5200 / 7000: loss 2.302097\n",
      "iteration 5300 / 7000: loss 2.302165\n",
      "iteration 5400 / 7000: loss 2.302112\n",
      "iteration 5500 / 7000: loss 2.302195\n",
      "iteration 5600 / 7000: loss 2.301881\n",
      "iteration 5700 / 7000: loss 2.302188\n",
      "iteration 5800 / 7000: loss 2.301926\n",
      "iteration 5900 / 7000: loss 2.302180\n",
      "iteration 6000 / 7000: loss 2.302024\n",
      "iteration 6100 / 7000: loss 2.302024\n",
      "iteration 6200 / 7000: loss 2.301879\n",
      "iteration 6300 / 7000: loss 2.302003\n",
      "iteration 6400 / 7000: loss 2.301996\n",
      "iteration 6500 / 7000: loss 2.302202\n",
      "iteration 6600 / 7000: loss 2.302153\n",
      "iteration 6700 / 7000: loss 2.302091\n",
      "iteration 6800 / 7000: loss 2.302234\n",
      "iteration 6900 / 7000: loss 2.301931\n",
      "new acc  0.253\n",
      "lr, rs 1e-05 2000.0\n",
      "iteration 0 / 7000: loss 455.853320\n",
      "iteration 100 / 7000: loss 2.302517\n",
      "iteration 200 / 7000: loss 2.302511\n",
      "iteration 300 / 7000: loss 2.302521\n",
      "iteration 400 / 7000: loss 2.302537\n",
      "iteration 500 / 7000: loss 2.302545\n",
      "iteration 600 / 7000: loss 2.302550\n",
      "iteration 700 / 7000: loss 2.302528\n",
      "iteration 800 / 7000: loss 2.302525\n",
      "iteration 900 / 7000: loss 2.302520\n",
      "iteration 1000 / 7000: loss 2.302514\n",
      "iteration 1100 / 7000: loss 2.302546\n",
      "iteration 1200 / 7000: loss 2.302520\n",
      "iteration 1300 / 7000: loss 2.302532\n",
      "iteration 1400 / 7000: loss 2.302528\n",
      "iteration 1500 / 7000: loss 2.302540\n",
      "iteration 1600 / 7000: loss 2.302567\n",
      "iteration 1700 / 7000: loss 2.302538\n",
      "iteration 1800 / 7000: loss 2.302542\n",
      "iteration 1900 / 7000: loss 2.302541\n",
      "iteration 2000 / 7000: loss 2.302529\n",
      "iteration 2100 / 7000: loss 2.302555\n",
      "iteration 2200 / 7000: loss 2.302521\n",
      "iteration 2300 / 7000: loss 2.302511\n",
      "iteration 2400 / 7000: loss 2.302518\n",
      "iteration 2500 / 7000: loss 2.302503\n",
      "iteration 2600 / 7000: loss 2.302521\n",
      "iteration 2700 / 7000: loss 2.302535\n",
      "iteration 2800 / 7000: loss 2.302528\n",
      "iteration 2900 / 7000: loss 2.302526\n",
      "iteration 3000 / 7000: loss 2.302538\n",
      "iteration 3100 / 7000: loss 2.302537\n",
      "iteration 3200 / 7000: loss 2.302546\n",
      "iteration 3300 / 7000: loss 2.302528\n",
      "iteration 3400 / 7000: loss 2.302524\n",
      "iteration 3500 / 7000: loss 2.302523\n",
      "iteration 3600 / 7000: loss 2.302543\n",
      "iteration 3700 / 7000: loss 2.302569\n",
      "iteration 3800 / 7000: loss 2.302540\n",
      "iteration 3900 / 7000: loss 2.302520\n",
      "iteration 4000 / 7000: loss 2.302536\n",
      "iteration 4100 / 7000: loss 2.302559\n",
      "iteration 4200 / 7000: loss 2.302553\n",
      "iteration 4300 / 7000: loss 2.302544\n",
      "iteration 4400 / 7000: loss 2.302545\n",
      "iteration 4500 / 7000: loss 2.302511\n",
      "iteration 4600 / 7000: loss 2.302525\n",
      "iteration 4700 / 7000: loss 2.302526\n",
      "iteration 4800 / 7000: loss 2.302540\n",
      "iteration 4900 / 7000: loss 2.302538\n",
      "iteration 5000 / 7000: loss 2.302521\n",
      "iteration 5100 / 7000: loss 2.302537\n",
      "iteration 5200 / 7000: loss 2.302546\n",
      "iteration 5300 / 7000: loss 2.302516\n",
      "iteration 5400 / 7000: loss 2.302527\n",
      "iteration 5500 / 7000: loss 2.302548\n",
      "iteration 5600 / 7000: loss 2.302549\n",
      "iteration 5700 / 7000: loss 2.302523\n",
      "iteration 5800 / 7000: loss 2.302522\n",
      "iteration 5900 / 7000: loss 2.302548\n",
      "iteration 6000 / 7000: loss 2.302525\n",
      "iteration 6100 / 7000: loss 2.302500\n",
      "iteration 6200 / 7000: loss 2.302548\n",
      "iteration 6300 / 7000: loss 2.302531\n",
      "iteration 6400 / 7000: loss 2.302544\n",
      "iteration 6500 / 7000: loss 2.302531\n",
      "iteration 6600 / 7000: loss 2.302533\n",
      "iteration 6700 / 7000: loss 2.302531\n",
      "iteration 6800 / 7000: loss 2.302568\n",
      "iteration 6900 / 7000: loss 2.302516\n",
      "new acc  0.257\n",
      "lr, rs 1e-05 15000.0\n",
      "iteration 0 / 7000: loss 33.357758\n",
      "iteration 100 / 7000: loss 2.301939\n",
      "iteration 200 / 7000: loss 2.301897\n",
      "iteration 300 / 7000: loss 2.301755\n",
      "iteration 400 / 7000: loss 2.301791\n",
      "iteration 500 / 7000: loss 2.301515\n",
      "iteration 600 / 7000: loss 2.301718\n",
      "iteration 700 / 7000: loss 2.301975\n",
      "iteration 800 / 7000: loss 2.301712\n",
      "iteration 900 / 7000: loss 2.301903\n",
      "iteration 1000 / 7000: loss 2.302069\n",
      "iteration 1100 / 7000: loss 2.301675\n",
      "iteration 1200 / 7000: loss 2.301786\n",
      "iteration 1300 / 7000: loss 2.301537\n",
      "iteration 1400 / 7000: loss 2.301267\n",
      "iteration 1500 / 7000: loss 2.301769\n",
      "iteration 1600 / 7000: loss 2.301940\n",
      "iteration 1700 / 7000: loss 2.301678\n",
      "iteration 1800 / 7000: loss 2.301644\n",
      "iteration 1900 / 7000: loss 2.301564\n",
      "iteration 2000 / 7000: loss 2.301657\n",
      "iteration 2100 / 7000: loss 2.301705\n",
      "iteration 2200 / 7000: loss 2.301266\n",
      "iteration 2300 / 7000: loss 2.301746\n",
      "iteration 2400 / 7000: loss 2.301745\n",
      "iteration 2500 / 7000: loss 2.302155\n",
      "iteration 2600 / 7000: loss 2.302020\n",
      "iteration 2700 / 7000: loss 2.301334\n",
      "iteration 2800 / 7000: loss 2.301985\n",
      "iteration 2900 / 7000: loss 2.301967\n",
      "iteration 3000 / 7000: loss 2.301538\n",
      "iteration 3100 / 7000: loss 2.301757\n",
      "iteration 3200 / 7000: loss 2.301768\n",
      "iteration 3300 / 7000: loss 2.302071\n",
      "iteration 3400 / 7000: loss 2.301519\n",
      "iteration 3500 / 7000: loss 2.301611\n",
      "iteration 3600 / 7000: loss 2.301607\n",
      "iteration 3700 / 7000: loss 2.301719\n",
      "iteration 3800 / 7000: loss 2.302173\n",
      "iteration 3900 / 7000: loss 2.301914\n",
      "iteration 4000 / 7000: loss 2.301700\n",
      "iteration 4100 / 7000: loss 2.301725\n",
      "iteration 4200 / 7000: loss 2.301819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4300 / 7000: loss 2.301786\n",
      "iteration 4400 / 7000: loss 2.301759\n",
      "iteration 4500 / 7000: loss 2.301740\n",
      "iteration 4600 / 7000: loss 2.302010\n",
      "iteration 4700 / 7000: loss 2.301800\n",
      "iteration 4800 / 7000: loss 2.301558\n",
      "iteration 4900 / 7000: loss 2.301938\n",
      "iteration 5000 / 7000: loss 2.301585\n",
      "iteration 5100 / 7000: loss 2.301993\n",
      "iteration 5200 / 7000: loss 2.301828\n",
      "iteration 5300 / 7000: loss 2.301947\n",
      "iteration 5400 / 7000: loss 2.301439\n",
      "iteration 5500 / 7000: loss 2.301779\n",
      "iteration 5600 / 7000: loss 2.301633\n",
      "iteration 5700 / 7000: loss 2.301698\n",
      "iteration 5800 / 7000: loss 2.301717\n",
      "iteration 5900 / 7000: loss 2.301443\n",
      "iteration 6000 / 7000: loss 2.302065\n",
      "iteration 6100 / 7000: loss 2.301380\n",
      "iteration 6200 / 7000: loss 2.301704\n",
      "iteration 6300 / 7000: loss 2.301895\n",
      "iteration 6400 / 7000: loss 2.301900\n",
      "iteration 6500 / 7000: loss 2.301993\n",
      "iteration 6600 / 7000: loss 2.301950\n",
      "iteration 6700 / 7000: loss 2.301811\n",
      "iteration 6800 / 7000: loss 2.302188\n",
      "iteration 6900 / 7000: loss 2.301804\n",
      "new acc  0.262\n",
      "lr, rs 5e-05 1000.0\n",
      "iteration 0 / 7000: loss 63.484221\n",
      "iteration 100 / 7000: loss 2.301862\n",
      "iteration 200 / 7000: loss 2.302195\n",
      "iteration 300 / 7000: loss 2.302273\n",
      "iteration 400 / 7000: loss 2.302281\n",
      "iteration 500 / 7000: loss 2.302284\n",
      "iteration 600 / 7000: loss 2.302075\n",
      "iteration 700 / 7000: loss 2.302201\n",
      "iteration 800 / 7000: loss 2.302171\n",
      "iteration 900 / 7000: loss 2.302075\n",
      "iteration 1000 / 7000: loss 2.302153\n",
      "iteration 1100 / 7000: loss 2.302005\n",
      "iteration 1200 / 7000: loss 2.301909\n",
      "iteration 1300 / 7000: loss 2.302254\n",
      "iteration 1400 / 7000: loss 2.302289\n",
      "iteration 1500 / 7000: loss 2.302296\n",
      "iteration 1600 / 7000: loss 2.302289\n",
      "iteration 1700 / 7000: loss 2.302180\n",
      "iteration 1800 / 7000: loss 2.302189\n",
      "iteration 1900 / 7000: loss 2.302096\n",
      "iteration 2000 / 7000: loss 2.302242\n",
      "iteration 2100 / 7000: loss 2.302111\n",
      "iteration 2200 / 7000: loss 2.302396\n",
      "iteration 2300 / 7000: loss 2.302094\n",
      "iteration 2400 / 7000: loss 2.302165\n",
      "iteration 2500 / 7000: loss 2.302225\n",
      "iteration 2600 / 7000: loss 2.302281\n",
      "iteration 2700 / 7000: loss 2.302315\n",
      "iteration 2800 / 7000: loss 2.301970\n",
      "iteration 2900 / 7000: loss 2.302070\n",
      "iteration 3000 / 7000: loss 2.302549\n",
      "iteration 3100 / 7000: loss 2.302052\n",
      "iteration 3200 / 7000: loss 2.302230\n",
      "iteration 3300 / 7000: loss 2.302237\n",
      "iteration 3400 / 7000: loss 2.302035\n",
      "iteration 3500 / 7000: loss 2.302154\n",
      "iteration 3600 / 7000: loss 2.302193\n",
      "iteration 3700 / 7000: loss 2.302027\n",
      "iteration 3800 / 7000: loss 2.302036\n",
      "iteration 3900 / 7000: loss 2.302143\n",
      "iteration 4000 / 7000: loss 2.302193\n",
      "iteration 4100 / 7000: loss 2.302280\n",
      "iteration 4200 / 7000: loss 2.302210\n",
      "iteration 4300 / 7000: loss 2.302156\n",
      "iteration 4400 / 7000: loss 2.302431\n",
      "iteration 4500 / 7000: loss 2.302412\n",
      "iteration 4600 / 7000: loss 2.302186\n",
      "iteration 4700 / 7000: loss 2.302174\n",
      "iteration 4800 / 7000: loss 2.302149\n",
      "iteration 4900 / 7000: loss 2.302416\n",
      "iteration 5000 / 7000: loss 2.302154\n",
      "iteration 5100 / 7000: loss 2.302262\n",
      "iteration 5200 / 7000: loss 2.302170\n",
      "iteration 5300 / 7000: loss 2.302156\n",
      "iteration 5400 / 7000: loss 2.302105\n",
      "iteration 5500 / 7000: loss 2.302137\n",
      "iteration 5600 / 7000: loss 2.302146\n",
      "iteration 5700 / 7000: loss 2.302046\n",
      "iteration 5800 / 7000: loss 2.302195\n",
      "iteration 5900 / 7000: loss 2.302156\n",
      "iteration 6000 / 7000: loss 2.302003\n",
      "iteration 6100 / 7000: loss 2.302100\n",
      "iteration 6200 / 7000: loss 2.302205\n",
      "iteration 6300 / 7000: loss 2.302177\n",
      "iteration 6400 / 7000: loss 2.302338\n",
      "iteration 6500 / 7000: loss 2.302171\n",
      "iteration 6600 / 7000: loss 2.302043\n",
      "iteration 6700 / 7000: loss 2.302203\n",
      "iteration 6800 / 7000: loss 2.302262\n",
      "iteration 6900 / 7000: loss 2.302219\n",
      "new acc  0.249\n",
      "lr, rs 5e-05 2000.0\n",
      "iteration 0 / 7000: loss 466.674694\n",
      "iteration 100 / 7000: loss 2.302620\n",
      "iteration 200 / 7000: loss 2.302610\n",
      "iteration 300 / 7000: loss 2.302661\n",
      "iteration 400 / 7000: loss 2.302669\n",
      "iteration 500 / 7000: loss 2.302608\n",
      "iteration 600 / 7000: loss 2.302703\n",
      "iteration 700 / 7000: loss 2.302642\n",
      "iteration 800 / 7000: loss 2.302666\n",
      "iteration 900 / 7000: loss 2.302624\n",
      "iteration 1000 / 7000: loss 2.302645\n",
      "iteration 1100 / 7000: loss 2.302643\n",
      "iteration 1200 / 7000: loss 2.302669\n",
      "iteration 1300 / 7000: loss 2.302772\n",
      "iteration 1400 / 7000: loss 2.302643\n",
      "iteration 1500 / 7000: loss 2.302714\n",
      "iteration 1600 / 7000: loss 2.302606\n",
      "iteration 1700 / 7000: loss 2.302625\n",
      "iteration 1800 / 7000: loss 2.302679\n",
      "iteration 1900 / 7000: loss 2.302645\n",
      "iteration 2000 / 7000: loss 2.302606\n",
      "iteration 2100 / 7000: loss 2.302605\n",
      "iteration 2200 / 7000: loss 2.302658\n",
      "iteration 2300 / 7000: loss 2.302666\n",
      "iteration 2400 / 7000: loss 2.302618\n",
      "iteration 2500 / 7000: loss 2.302649\n",
      "iteration 2600 / 7000: loss 2.302678\n",
      "iteration 2700 / 7000: loss 2.302667\n",
      "iteration 2800 / 7000: loss 2.302642\n",
      "iteration 2900 / 7000: loss 2.302636\n",
      "iteration 3000 / 7000: loss 2.302633\n",
      "iteration 3100 / 7000: loss 2.302604\n",
      "iteration 3200 / 7000: loss 2.302628\n",
      "iteration 3300 / 7000: loss 2.302607\n",
      "iteration 3400 / 7000: loss 2.302693\n",
      "iteration 3500 / 7000: loss 2.302623\n",
      "iteration 3600 / 7000: loss 2.302690\n",
      "iteration 3700 / 7000: loss 2.302637\n",
      "iteration 3800 / 7000: loss 2.302646\n",
      "iteration 3900 / 7000: loss 2.302661\n",
      "iteration 4000 / 7000: loss 2.302644\n",
      "iteration 4100 / 7000: loss 2.302620\n",
      "iteration 4200 / 7000: loss 2.302626\n",
      "iteration 4300 / 7000: loss 2.302698\n",
      "iteration 4400 / 7000: loss 2.302644\n",
      "iteration 4500 / 7000: loss 2.302619\n",
      "iteration 4600 / 7000: loss 2.302663\n",
      "iteration 4700 / 7000: loss 2.302643\n",
      "iteration 4800 / 7000: loss 2.302698\n",
      "iteration 4900 / 7000: loss 2.302611\n",
      "iteration 5000 / 7000: loss 2.302675\n",
      "iteration 5100 / 7000: loss 2.302659\n",
      "iteration 5200 / 7000: loss 2.302603\n",
      "iteration 5300 / 7000: loss 2.302627\n",
      "iteration 5400 / 7000: loss 2.302617\n",
      "iteration 5500 / 7000: loss 2.302637\n",
      "iteration 5600 / 7000: loss 2.302616\n",
      "iteration 5700 / 7000: loss 2.302651\n",
      "iteration 5800 / 7000: loss 2.302630\n",
      "iteration 5900 / 7000: loss 2.302608\n",
      "iteration 6000 / 7000: loss 2.302638\n",
      "iteration 6100 / 7000: loss 2.302616\n",
      "iteration 6200 / 7000: loss 2.302657\n",
      "iteration 6300 / 7000: loss 2.302667\n",
      "iteration 6400 / 7000: loss 2.302637\n",
      "iteration 6500 / 7000: loss 2.302633\n",
      "iteration 6600 / 7000: loss 2.302655\n",
      "iteration 6700 / 7000: loss 2.302609\n",
      "iteration 6800 / 7000: loss 2.302637\n",
      "iteration 6900 / 7000: loss 2.302644\n",
      "new acc  0.182\n",
      "lr, rs 5e-05 15000.0\n",
      "iteration 0 / 7000: loss 32.700164\n",
      "iteration 100 / 7000: loss 2.302231\n",
      "iteration 200 / 7000: loss 2.302664\n",
      "iteration 300 / 7000: loss 2.302490\n",
      "iteration 400 / 7000: loss 2.302392\n",
      "iteration 500 / 7000: loss 2.302628\n",
      "iteration 600 / 7000: loss 2.302160\n",
      "iteration 700 / 7000: loss 2.302049\n",
      "iteration 800 / 7000: loss 2.301988\n",
      "iteration 900 / 7000: loss 2.301927\n",
      "iteration 1000 / 7000: loss 2.302193\n",
      "iteration 1100 / 7000: loss 2.302746\n",
      "iteration 1200 / 7000: loss 2.302126\n",
      "iteration 1300 / 7000: loss 2.302843\n",
      "iteration 1400 / 7000: loss 2.301993\n",
      "iteration 1500 / 7000: loss 2.302689\n",
      "iteration 1600 / 7000: loss 2.302170\n",
      "iteration 1700 / 7000: loss 2.302263\n",
      "iteration 1800 / 7000: loss 2.301932\n",
      "iteration 1900 / 7000: loss 2.302785\n",
      "iteration 2000 / 7000: loss 2.302509\n",
      "iteration 2100 / 7000: loss 2.302308\n",
      "iteration 2200 / 7000: loss 2.302100\n",
      "iteration 2300 / 7000: loss 2.302422\n",
      "iteration 2400 / 7000: loss 2.302333\n",
      "iteration 2500 / 7000: loss 2.302305\n",
      "iteration 2600 / 7000: loss 2.301875\n",
      "iteration 2700 / 7000: loss 2.302702\n",
      "iteration 2800 / 7000: loss 2.302459\n",
      "iteration 2900 / 7000: loss 2.302440\n",
      "iteration 3000 / 7000: loss 2.302121\n",
      "iteration 3100 / 7000: loss 2.302375\n",
      "iteration 3200 / 7000: loss 2.302527\n",
      "iteration 3300 / 7000: loss 2.302182\n",
      "iteration 3400 / 7000: loss 2.302540\n",
      "iteration 3500 / 7000: loss 2.302167\n",
      "iteration 3600 / 7000: loss 2.302465\n",
      "iteration 3700 / 7000: loss 2.302312\n",
      "iteration 3800 / 7000: loss 2.302432\n",
      "iteration 3900 / 7000: loss 2.302881\n",
      "iteration 4000 / 7000: loss 2.302536\n",
      "iteration 4100 / 7000: loss 2.302618\n",
      "iteration 4200 / 7000: loss 2.302778\n",
      "iteration 4300 / 7000: loss 2.302515\n",
      "iteration 4400 / 7000: loss 2.302401\n",
      "iteration 4500 / 7000: loss 2.302460\n",
      "iteration 4600 / 7000: loss 2.301788\n",
      "iteration 4700 / 7000: loss 2.302411\n",
      "iteration 4800 / 7000: loss 2.302515\n",
      "iteration 4900 / 7000: loss 2.302943\n",
      "iteration 5000 / 7000: loss 2.302031\n",
      "iteration 5100 / 7000: loss 2.301947\n",
      "iteration 5200 / 7000: loss 2.302506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5300 / 7000: loss 2.302167\n",
      "iteration 5400 / 7000: loss 2.302559\n",
      "iteration 5500 / 7000: loss 2.302637\n",
      "iteration 5600 / 7000: loss 2.302890\n",
      "iteration 5700 / 7000: loss 2.302335\n",
      "iteration 5800 / 7000: loss 2.302315\n",
      "iteration 5900 / 7000: loss 2.302925\n",
      "iteration 6000 / 7000: loss 2.302382\n",
      "iteration 6100 / 7000: loss 2.301848\n",
      "iteration 6200 / 7000: loss 2.302622\n",
      "iteration 6300 / 7000: loss 2.302231\n",
      "iteration 6400 / 7000: loss 2.302838\n",
      "iteration 6500 / 7000: loss 2.302214\n",
      "iteration 6600 / 7000: loss 2.302365\n",
      "iteration 6700 / 7000: loss 2.302361\n",
      "iteration 6800 / 7000: loss 2.302182\n",
      "iteration 6900 / 7000: loss 2.301947\n",
      "new acc  0.207\n",
      "lr, rs 0.0005 1000.0\n",
      "iteration 0 / 7000: loss 64.292417\n",
      "iteration 100 / 7000: loss 64.531767\n",
      "iteration 200 / 7000: loss 65.298232\n",
      "iteration 300 / 7000: loss 67.512992\n",
      "iteration 400 / 7000: loss 76.687151\n",
      "iteration 500 / 7000: loss 115.011477\n",
      "iteration 600 / 7000: loss 273.174078\n",
      "iteration 700 / 7000: loss 744.835265\n",
      "iteration 800 / 7000: loss 1650.094260\n",
      "iteration 900 / 7000: loss 3046.148527\n",
      "iteration 1000 / 7000: loss 4904.207367\n",
      "iteration 1100 / 7000: loss 7246.163193\n",
      "iteration 1200 / 7000: loss 10049.715044\n",
      "iteration 1300 / 7000: loss 13352.430354\n",
      "iteration 1400 / 7000: loss 17127.267914\n",
      "iteration 1500 / 7000: loss 21379.821769\n",
      "iteration 1600 / 7000: loss 26122.690488\n",
      "iteration 1700 / 7000: loss 31316.844147\n",
      "iteration 1800 / 7000: loss 36992.734937\n",
      "iteration 1900 / 7000: loss 43159.966498\n",
      "iteration 2000 / 7000: loss 49900.225001\n",
      "iteration 2100 / 7000: loss 57112.204682\n",
      "iteration 2200 / 7000: loss 64758.830106\n",
      "iteration 2300 / 7000: loss 72881.626537\n",
      "iteration 2400 / 7000: loss 81547.512707\n",
      "iteration 2500 / 7000: loss 90646.107140\n",
      "iteration 2600 / 7000: loss 100246.301226\n",
      "iteration 2700 / 7000: loss 110361.724893\n",
      "iteration 2800 / 7000: loss 120955.325434\n",
      "iteration 2900 / 7000: loss 131817.598230\n",
      "iteration 3000 / 7000: loss 143244.838722\n",
      "iteration 3100 / 7000: loss 155290.153687\n",
      "iteration 3200 / 7000: loss 167571.693748\n",
      "iteration 3300 / 7000: loss 180411.144416\n",
      "iteration 3400 / 7000: loss 193747.146242\n",
      "iteration 3500 / 7000: loss 207679.587788\n",
      "iteration 3600 / 7000: loss 221960.409136\n",
      "iteration 3700 / 7000: loss 236662.020650\n",
      "iteration 3800 / 7000: loss 252027.871112\n",
      "iteration 3900 / 7000: loss 267828.647836\n",
      "iteration 4000 / 7000: loss 284161.376520\n",
      "iteration 4100 / 7000: loss 300781.341783\n",
      "iteration 4200 / 7000: loss 318036.241594\n",
      "iteration 4300 / 7000: loss 335766.734250\n",
      "iteration 4400 / 7000: loss 354234.737842\n",
      "iteration 4500 / 7000: loss 372970.978227\n",
      "iteration 4600 / 7000: loss 392012.437896\n",
      "iteration 4700 / 7000: loss 411473.002086\n",
      "iteration 4800 / 7000: loss 431548.201877\n",
      "iteration 4900 / 7000: loss 452144.247002\n",
      "iteration 5000 / 7000: loss 473241.286230\n",
      "iteration 5100 / 7000: loss 494665.212090\n",
      "iteration 5200 / 7000: loss 516672.625839\n",
      "iteration 5300 / 7000: loss 539313.445505\n",
      "iteration 5400 / 7000: loss 562183.948944\n",
      "iteration 5500 / 7000: loss 585756.700737\n",
      "iteration 5600 / 7000: loss 609629.907036\n",
      "iteration 5700 / 7000: loss 633920.183657\n",
      "iteration 5800 / 7000: loss 658792.119939\n",
      "iteration 5900 / 7000: loss 684085.066709\n",
      "iteration 6000 / 7000: loss 710153.680792\n",
      "iteration 6100 / 7000: loss 736441.526982\n",
      "iteration 6200 / 7000: loss 763470.538024\n",
      "iteration 6300 / 7000: loss 791057.490013\n",
      "iteration 6400 / 7000: loss inf\n",
      "iteration 6500 / 7000: loss inf\n",
      "iteration 6600 / 7000: loss inf\n",
      "iteration 6700 / 7000: loss inf\n",
      "iteration 6800 / 7000: loss 935086.569178\n",
      "iteration 6900 / 7000: loss inf\n",
      "new acc  0.11\n",
      "lr, rs 0.0005 2000.0\n",
      "iteration 0 / 7000: loss 459.543719\n",
      "iteration 100 / 7000: loss nan\n",
      "iteration 200 / 7000: loss nan\n",
      "iteration 300 / 7000: loss nan\n",
      "iteration 400 / 7000: loss nan\n",
      "iteration 500 / 7000: loss nan\n",
      "iteration 600 / 7000: loss nan\n",
      "iteration 700 / 7000: loss nan\n",
      "iteration 800 / 7000: loss nan\n",
      "iteration 900 / 7000: loss nan\n",
      "iteration 1000 / 7000: loss nan\n",
      "iteration 1100 / 7000: loss nan\n",
      "iteration 1200 / 7000: loss nan\n",
      "iteration 1300 / 7000: loss nan\n",
      "iteration 1400 / 7000: loss nan\n",
      "iteration 1500 / 7000: loss nan\n",
      "iteration 1600 / 7000: loss nan\n",
      "iteration 1700 / 7000: loss nan\n",
      "iteration 1800 / 7000: loss nan\n",
      "iteration 1900 / 7000: loss nan\n",
      "iteration 2000 / 7000: loss nan\n",
      "iteration 2100 / 7000: loss nan\n",
      "iteration 2200 / 7000: loss nan\n",
      "iteration 2300 / 7000: loss nan\n",
      "iteration 2400 / 7000: loss nan\n",
      "iteration 2500 / 7000: loss nan\n",
      "iteration 2600 / 7000: loss nan\n",
      "iteration 2700 / 7000: loss nan\n",
      "iteration 2800 / 7000: loss nan\n",
      "iteration 2900 / 7000: loss nan\n",
      "iteration 3000 / 7000: loss nan\n",
      "iteration 3100 / 7000: loss nan\n",
      "iteration 3200 / 7000: loss nan\n",
      "iteration 3300 / 7000: loss nan\n",
      "iteration 3400 / 7000: loss nan\n",
      "iteration 3500 / 7000: loss nan\n",
      "iteration 3600 / 7000: loss nan\n",
      "iteration 3700 / 7000: loss nan\n",
      "iteration 3800 / 7000: loss nan\n",
      "iteration 3900 / 7000: loss nan\n",
      "iteration 4000 / 7000: loss nan\n",
      "iteration 4100 / 7000: loss nan\n",
      "iteration 4200 / 7000: loss nan\n",
      "iteration 4300 / 7000: loss nan\n",
      "iteration 4400 / 7000: loss nan\n",
      "iteration 4500 / 7000: loss nan\n",
      "iteration 4600 / 7000: loss nan\n",
      "iteration 4700 / 7000: loss nan\n",
      "iteration 4800 / 7000: loss nan\n",
      "iteration 4900 / 7000: loss nan\n",
      "iteration 5000 / 7000: loss nan\n",
      "iteration 5100 / 7000: loss nan\n",
      "iteration 5200 / 7000: loss nan\n",
      "iteration 5300 / 7000: loss nan\n",
      "iteration 5400 / 7000: loss nan\n",
      "iteration 5500 / 7000: loss nan\n",
      "iteration 5600 / 7000: loss nan\n",
      "iteration 5700 / 7000: loss nan\n",
      "iteration 5800 / 7000: loss nan\n",
      "iteration 5900 / 7000: loss nan\n",
      "iteration 6000 / 7000: loss nan\n",
      "iteration 6100 / 7000: loss nan\n",
      "iteration 6200 / 7000: loss nan\n",
      "iteration 6300 / 7000: loss nan\n",
      "iteration 6400 / 7000: loss nan\n",
      "iteration 6500 / 7000: loss nan\n",
      "iteration 6600 / 7000: loss nan\n",
      "iteration 6700 / 7000: loss nan\n",
      "iteration 6800 / 7000: loss nan\n",
      "iteration 6900 / 7000: loss nan\n",
      "new acc  0.087\n",
      "lr, rs 0.0005 15000.0\n",
      "iteration 0 / 7000: loss 33.129877\n",
      "iteration 100 / 7000: loss 34.017156\n",
      "iteration 200 / 7000: loss 43.075329\n",
      "iteration 300 / 7000: loss 163.561632\n",
      "iteration 400 / 7000: loss 783.385897\n",
      "iteration 500 / 7000: loss 2281.548389\n",
      "iteration 600 / 7000: loss 4757.496567\n",
      "iteration 700 / 7000: loss 8191.911239\n",
      "iteration 800 / 7000: loss 12545.687636\n",
      "iteration 900 / 7000: loss 17827.703326\n",
      "iteration 1000 / 7000: loss 24127.070959\n",
      "iteration 1100 / 7000: loss 31287.834501\n",
      "iteration 1200 / 7000: loss 39399.291541\n",
      "iteration 1300 / 7000: loss 48598.404224\n",
      "iteration 1400 / 7000: loss 58757.840945\n",
      "iteration 1500 / 7000: loss 69712.171248\n",
      "iteration 1600 / 7000: loss 81774.649044\n",
      "iteration 1700 / 7000: loss 94851.274533\n",
      "iteration 1800 / 7000: loss 108906.795152\n",
      "iteration 1900 / 7000: loss 123814.888955\n",
      "iteration 2000 / 7000: loss 139867.191846\n",
      "iteration 2100 / 7000: loss 156739.564997\n",
      "iteration 2200 / 7000: loss 174547.022840\n",
      "iteration 2300 / 7000: loss 193229.120063\n",
      "iteration 2400 / 7000: loss 212879.191547\n",
      "iteration 2500 / 7000: loss inf\n",
      "iteration 2600 / 7000: loss 255255.539291\n",
      "iteration 2700 / 7000: loss inf\n",
      "iteration 2800 / 7000: loss 301306.240220\n",
      "iteration 2900 / 7000: loss inf\n",
      "iteration 3000 / 7000: loss 351232.665346\n",
      "iteration 3100 / 7000: loss 377750.369907\n",
      "iteration 3200 / 7000: loss 405112.897323\n",
      "iteration 3300 / 7000: loss inf\n",
      "iteration 3400 / 7000: loss inf\n",
      "iteration 3500 / 7000: loss 493030.004270\n",
      "iteration 3600 / 7000: loss 524237.482212\n",
      "iteration 3700 / 7000: loss inf\n",
      "iteration 3800 / 7000: loss inf\n",
      "iteration 3900 / 7000: loss 623936.756841\n",
      "iteration 4000 / 7000: loss 658675.409282\n",
      "iteration 4100 / 7000: loss nan\n",
      "iteration 4200 / 7000: loss nan\n",
      "iteration 4300 / 7000: loss nan\n",
      "iteration 4400 / 7000: loss nan\n",
      "iteration 4500 / 7000: loss nan\n",
      "iteration 4600 / 7000: loss nan\n",
      "iteration 4700 / 7000: loss nan\n",
      "iteration 4800 / 7000: loss nan\n",
      "iteration 4900 / 7000: loss nan\n",
      "iteration 5000 / 7000: loss nan\n",
      "iteration 5100 / 7000: loss nan\n",
      "iteration 5200 / 7000: loss nan\n",
      "iteration 5300 / 7000: loss nan\n",
      "iteration 5400 / 7000: loss nan\n",
      "iteration 5500 / 7000: loss nan\n",
      "iteration 5600 / 7000: loss nan\n",
      "iteration 5700 / 7000: loss nan\n",
      "iteration 5800 / 7000: loss nan\n",
      "iteration 5900 / 7000: loss nan\n",
      "iteration 6000 / 7000: loss nan\n",
      "iteration 6100 / 7000: loss nan\n",
      "iteration 6200 / 7000: loss nan\n",
      "iteration 6300 / 7000: loss nan\n",
      "iteration 6400 / 7000: loss nan\n",
      "iteration 6500 / 7000: loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6600 / 7000: loss nan\n",
      "iteration 6700 / 7000: loss nan\n",
      "iteration 6800 / 7000: loss nan\n",
      "iteration 6900 / 7000: loss nan\n",
      "new acc  0.087\n",
      "lr, rs 0.001 1000.0\n",
      "iteration 0 / 7000: loss 63.563473\n",
      "iteration 100 / 7000: loss nan\n",
      "iteration 200 / 7000: loss nan\n",
      "iteration 300 / 7000: loss nan\n",
      "iteration 400 / 7000: loss nan\n",
      "iteration 500 / 7000: loss nan\n",
      "iteration 600 / 7000: loss nan\n",
      "iteration 700 / 7000: loss nan\n",
      "iteration 800 / 7000: loss nan\n",
      "iteration 900 / 7000: loss nan\n",
      "iteration 1000 / 7000: loss nan\n",
      "iteration 1100 / 7000: loss nan\n",
      "iteration 1200 / 7000: loss nan\n",
      "iteration 1300 / 7000: loss nan\n",
      "iteration 1400 / 7000: loss nan\n",
      "iteration 1500 / 7000: loss nan\n",
      "iteration 1600 / 7000: loss nan\n",
      "iteration 1700 / 7000: loss nan\n",
      "iteration 1800 / 7000: loss nan\n",
      "iteration 1900 / 7000: loss nan\n",
      "iteration 2000 / 7000: loss nan\n",
      "iteration 2100 / 7000: loss nan\n",
      "iteration 2200 / 7000: loss nan\n",
      "iteration 2300 / 7000: loss nan\n",
      "iteration 2400 / 7000: loss nan\n",
      "iteration 2500 / 7000: loss nan\n",
      "iteration 2600 / 7000: loss nan\n",
      "iteration 2700 / 7000: loss nan\n",
      "iteration 2800 / 7000: loss nan\n",
      "iteration 2900 / 7000: loss nan\n",
      "iteration 3000 / 7000: loss nan\n",
      "iteration 3100 / 7000: loss nan\n",
      "iteration 3200 / 7000: loss nan\n",
      "iteration 3300 / 7000: loss nan\n",
      "iteration 3400 / 7000: loss nan\n",
      "iteration 3500 / 7000: loss nan\n",
      "iteration 3600 / 7000: loss nan\n",
      "iteration 3700 / 7000: loss nan\n",
      "iteration 3800 / 7000: loss nan\n",
      "iteration 3900 / 7000: loss nan\n",
      "iteration 4000 / 7000: loss nan\n",
      "iteration 4100 / 7000: loss nan\n",
      "iteration 4200 / 7000: loss nan\n",
      "iteration 4300 / 7000: loss nan\n",
      "iteration 4400 / 7000: loss nan\n",
      "iteration 4500 / 7000: loss nan\n",
      "iteration 4600 / 7000: loss nan\n",
      "iteration 4700 / 7000: loss nan\n",
      "iteration 4800 / 7000: loss nan\n",
      "iteration 4900 / 7000: loss nan\n",
      "iteration 5000 / 7000: loss nan\n",
      "iteration 5100 / 7000: loss nan\n",
      "iteration 5200 / 7000: loss nan\n",
      "iteration 5300 / 7000: loss nan\n",
      "iteration 5400 / 7000: loss nan\n",
      "iteration 5500 / 7000: loss nan\n",
      "iteration 5600 / 7000: loss nan\n",
      "iteration 5700 / 7000: loss nan\n",
      "iteration 5800 / 7000: loss nan\n",
      "iteration 5900 / 7000: loss nan\n",
      "iteration 6000 / 7000: loss nan\n",
      "iteration 6100 / 7000: loss nan\n",
      "iteration 6200 / 7000: loss nan\n",
      "iteration 6300 / 7000: loss nan\n",
      "iteration 6400 / 7000: loss nan\n",
      "iteration 6500 / 7000: loss nan\n",
      "iteration 6600 / 7000: loss nan\n",
      "iteration 6700 / 7000: loss nan\n",
      "iteration 6800 / 7000: loss nan\n",
      "iteration 6900 / 7000: loss nan\n",
      "new acc  0.087\n",
      "lr, rs 0.001 2000.0\n",
      "iteration 0 / 7000: loss 464.465881\n",
      "iteration 100 / 7000: loss nan\n",
      "iteration 200 / 7000: loss nan\n",
      "iteration 300 / 7000: loss nan\n",
      "iteration 400 / 7000: loss nan\n",
      "iteration 500 / 7000: loss nan\n",
      "iteration 600 / 7000: loss nan\n",
      "iteration 700 / 7000: loss nan\n",
      "iteration 800 / 7000: loss nan\n",
      "iteration 900 / 7000: loss nan\n",
      "iteration 1000 / 7000: loss nan\n",
      "iteration 1100 / 7000: loss nan\n",
      "iteration 1200 / 7000: loss nan\n",
      "iteration 1300 / 7000: loss nan\n",
      "iteration 1400 / 7000: loss nan\n",
      "iteration 1500 / 7000: loss nan\n",
      "iteration 1600 / 7000: loss nan\n",
      "iteration 1700 / 7000: loss nan\n",
      "iteration 1800 / 7000: loss nan\n",
      "iteration 1900 / 7000: loss nan\n",
      "iteration 2000 / 7000: loss nan\n",
      "iteration 2100 / 7000: loss nan\n",
      "iteration 2200 / 7000: loss nan\n",
      "iteration 2300 / 7000: loss nan\n",
      "iteration 2400 / 7000: loss nan\n",
      "iteration 2500 / 7000: loss nan\n",
      "iteration 2600 / 7000: loss nan\n",
      "iteration 2700 / 7000: loss nan\n",
      "iteration 2800 / 7000: loss nan\n",
      "iteration 2900 / 7000: loss nan\n",
      "iteration 3000 / 7000: loss nan\n",
      "iteration 3100 / 7000: loss nan\n",
      "iteration 3200 / 7000: loss nan\n",
      "iteration 3300 / 7000: loss nan\n",
      "iteration 3400 / 7000: loss nan\n",
      "iteration 3500 / 7000: loss nan\n",
      "iteration 3600 / 7000: loss nan\n",
      "iteration 3700 / 7000: loss nan\n",
      "iteration 3800 / 7000: loss nan\n",
      "iteration 3900 / 7000: loss nan\n",
      "iteration 4000 / 7000: loss nan\n",
      "iteration 4100 / 7000: loss nan\n",
      "iteration 4200 / 7000: loss nan\n",
      "iteration 4300 / 7000: loss nan\n",
      "iteration 4400 / 7000: loss nan\n",
      "iteration 4500 / 7000: loss nan\n",
      "iteration 4600 / 7000: loss nan\n",
      "iteration 4700 / 7000: loss nan\n",
      "iteration 4800 / 7000: loss nan\n",
      "iteration 4900 / 7000: loss nan\n",
      "iteration 5000 / 7000: loss nan\n",
      "iteration 5100 / 7000: loss nan\n",
      "iteration 5200 / 7000: loss nan\n",
      "iteration 5300 / 7000: loss nan\n",
      "iteration 5400 / 7000: loss nan\n",
      "iteration 5500 / 7000: loss nan\n",
      "iteration 5600 / 7000: loss nan\n",
      "iteration 5700 / 7000: loss nan\n",
      "iteration 5800 / 7000: loss nan\n",
      "iteration 5900 / 7000: loss nan\n",
      "iteration 6000 / 7000: loss nan\n",
      "iteration 6100 / 7000: loss nan\n",
      "iteration 6200 / 7000: loss nan\n",
      "iteration 6300 / 7000: loss nan\n",
      "iteration 6400 / 7000: loss nan\n",
      "iteration 6500 / 7000: loss nan\n",
      "iteration 6600 / 7000: loss nan\n",
      "iteration 6700 / 7000: loss nan\n",
      "iteration 6800 / 7000: loss nan\n",
      "iteration 6900 / 7000: loss nan\n",
      "new acc  0.087\n",
      "lr, rs 0.001 15000.0\n",
      "iteration 0 / 7000: loss 33.241668\n",
      "iteration 100 / 7000: loss nan\n",
      "iteration 200 / 7000: loss nan\n",
      "iteration 300 / 7000: loss nan\n",
      "iteration 400 / 7000: loss nan\n",
      "iteration 500 / 7000: loss nan\n",
      "iteration 600 / 7000: loss nan\n",
      "iteration 700 / 7000: loss nan\n",
      "iteration 800 / 7000: loss nan\n",
      "iteration 900 / 7000: loss nan\n",
      "iteration 1000 / 7000: loss nan\n",
      "iteration 1100 / 7000: loss nan\n",
      "iteration 1200 / 7000: loss nan\n",
      "iteration 1300 / 7000: loss nan\n",
      "iteration 1400 / 7000: loss nan\n",
      "iteration 1500 / 7000: loss nan\n",
      "iteration 1600 / 7000: loss nan\n",
      "iteration 1700 / 7000: loss nan\n",
      "iteration 1800 / 7000: loss nan\n",
      "iteration 1900 / 7000: loss nan\n",
      "iteration 2000 / 7000: loss nan\n",
      "iteration 2100 / 7000: loss nan\n",
      "iteration 2200 / 7000: loss nan\n",
      "iteration 2300 / 7000: loss nan\n",
      "iteration 2400 / 7000: loss nan\n",
      "iteration 2500 / 7000: loss nan\n",
      "iteration 2600 / 7000: loss nan\n",
      "iteration 2700 / 7000: loss nan\n",
      "iteration 2800 / 7000: loss nan\n",
      "iteration 2900 / 7000: loss nan\n",
      "iteration 3000 / 7000: loss nan\n",
      "iteration 3100 / 7000: loss nan\n",
      "iteration 3200 / 7000: loss nan\n",
      "iteration 3300 / 7000: loss nan\n",
      "iteration 3400 / 7000: loss nan\n",
      "iteration 3500 / 7000: loss nan\n",
      "iteration 3600 / 7000: loss nan\n",
      "iteration 3700 / 7000: loss nan\n",
      "iteration 3800 / 7000: loss nan\n",
      "iteration 3900 / 7000: loss nan\n",
      "iteration 4000 / 7000: loss nan\n",
      "iteration 4100 / 7000: loss nan\n",
      "iteration 4200 / 7000: loss nan\n",
      "iteration 4300 / 7000: loss nan\n",
      "iteration 4400 / 7000: loss nan\n",
      "iteration 4500 / 7000: loss nan\n",
      "iteration 4600 / 7000: loss nan\n",
      "iteration 4700 / 7000: loss nan\n",
      "iteration 4800 / 7000: loss nan\n",
      "iteration 4900 / 7000: loss nan\n",
      "iteration 5000 / 7000: loss nan\n",
      "iteration 5100 / 7000: loss nan\n",
      "iteration 5200 / 7000: loss nan\n",
      "iteration 5300 / 7000: loss nan\n",
      "iteration 5400 / 7000: loss nan\n",
      "iteration 5500 / 7000: loss nan\n",
      "iteration 5600 / 7000: loss nan\n",
      "iteration 5700 / 7000: loss nan\n",
      "iteration 5800 / 7000: loss nan\n",
      "iteration 5900 / 7000: loss nan\n",
      "iteration 6000 / 7000: loss nan\n",
      "iteration 6100 / 7000: loss nan\n",
      "iteration 6200 / 7000: loss nan\n",
      "iteration 6300 / 7000: loss nan\n",
      "iteration 6400 / 7000: loss nan\n",
      "iteration 6500 / 7000: loss nan\n",
      "iteration 6600 / 7000: loss nan\n",
      "iteration 6700 / 7000: loss nan\n",
      "iteration 6800 / 7000: loss nan\n",
      "iteration 6900 / 7000: loss nan\n",
      "new acc  0.087\n",
      "lr, rs 0.005 1000.0\n",
      "iteration 0 / 7000: loss 63.646220\n",
      "iteration 100 / 7000: loss nan\n",
      "iteration 200 / 7000: loss nan\n",
      "iteration 300 / 7000: loss nan\n",
      "iteration 400 / 7000: loss nan\n",
      "iteration 500 / 7000: loss nan\n",
      "iteration 600 / 7000: loss nan\n",
      "iteration 700 / 7000: loss nan\n",
      "iteration 800 / 7000: loss nan\n",
      "iteration 900 / 7000: loss nan\n",
      "iteration 1000 / 7000: loss nan\n",
      "iteration 1100 / 7000: loss nan\n",
      "iteration 1200 / 7000: loss nan\n",
      "iteration 1300 / 7000: loss nan\n",
      "iteration 1400 / 7000: loss nan\n",
      "iteration 1500 / 7000: loss nan\n",
      "iteration 1600 / 7000: loss nan\n",
      "iteration 1700 / 7000: loss nan\n",
      "iteration 1800 / 7000: loss nan\n",
      "iteration 1900 / 7000: loss nan\n",
      "iteration 2000 / 7000: loss nan\n",
      "iteration 2100 / 7000: loss nan\n",
      "iteration 2200 / 7000: loss nan\n",
      "iteration 2300 / 7000: loss nan\n",
      "iteration 2400 / 7000: loss nan\n",
      "iteration 2500 / 7000: loss nan\n",
      "iteration 2600 / 7000: loss nan\n",
      "iteration 2700 / 7000: loss nan\n",
      "iteration 2800 / 7000: loss nan\n",
      "iteration 2900 / 7000: loss nan\n",
      "iteration 3000 / 7000: loss nan\n",
      "iteration 3100 / 7000: loss nan\n",
      "iteration 3200 / 7000: loss nan\n",
      "iteration 3300 / 7000: loss nan\n",
      "iteration 3400 / 7000: loss nan\n",
      "iteration 3500 / 7000: loss nan\n",
      "iteration 3600 / 7000: loss nan\n",
      "iteration 3700 / 7000: loss nan\n",
      "iteration 3800 / 7000: loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3900 / 7000: loss nan\n",
      "iteration 4000 / 7000: loss nan\n",
      "iteration 4100 / 7000: loss nan\n",
      "iteration 4200 / 7000: loss nan\n",
      "iteration 4300 / 7000: loss nan\n",
      "iteration 4400 / 7000: loss nan\n",
      "iteration 4500 / 7000: loss nan\n",
      "iteration 4600 / 7000: loss nan\n",
      "iteration 4700 / 7000: loss nan\n",
      "iteration 4800 / 7000: loss nan\n",
      "iteration 4900 / 7000: loss nan\n",
      "iteration 5000 / 7000: loss nan\n",
      "iteration 5100 / 7000: loss nan\n",
      "iteration 5200 / 7000: loss nan\n",
      "iteration 5300 / 7000: loss nan\n",
      "iteration 5400 / 7000: loss nan\n",
      "iteration 5500 / 7000: loss nan\n",
      "iteration 5600 / 7000: loss nan\n",
      "iteration 5700 / 7000: loss nan\n",
      "iteration 5800 / 7000: loss nan\n",
      "iteration 5900 / 7000: loss nan\n",
      "iteration 6000 / 7000: loss nan\n",
      "iteration 6100 / 7000: loss nan\n",
      "iteration 6200 / 7000: loss nan\n",
      "iteration 6300 / 7000: loss nan\n",
      "iteration 6400 / 7000: loss nan\n",
      "iteration 6500 / 7000: loss nan\n",
      "iteration 6600 / 7000: loss nan\n",
      "iteration 6700 / 7000: loss nan\n",
      "iteration 6800 / 7000: loss nan\n",
      "iteration 6900 / 7000: loss nan\n",
      "new acc  0.087\n",
      "lr, rs 0.005 2000.0\n",
      "iteration 0 / 7000: loss 465.366240\n",
      "iteration 100 / 7000: loss nan\n",
      "iteration 200 / 7000: loss nan\n",
      "iteration 300 / 7000: loss nan\n",
      "iteration 400 / 7000: loss nan\n",
      "iteration 500 / 7000: loss nan\n",
      "iteration 600 / 7000: loss nan\n",
      "iteration 700 / 7000: loss nan\n",
      "iteration 800 / 7000: loss nan\n",
      "iteration 900 / 7000: loss nan\n",
      "iteration 1000 / 7000: loss nan\n",
      "iteration 1100 / 7000: loss nan\n",
      "iteration 1200 / 7000: loss nan\n",
      "iteration 1300 / 7000: loss nan\n",
      "iteration 1400 / 7000: loss nan\n",
      "iteration 1500 / 7000: loss nan\n",
      "iteration 1600 / 7000: loss nan\n",
      "iteration 1700 / 7000: loss nan\n",
      "iteration 1800 / 7000: loss nan\n",
      "iteration 1900 / 7000: loss nan\n",
      "iteration 2000 / 7000: loss nan\n",
      "iteration 2100 / 7000: loss nan\n",
      "iteration 2200 / 7000: loss nan\n",
      "iteration 2300 / 7000: loss nan\n",
      "iteration 2400 / 7000: loss nan\n",
      "iteration 2500 / 7000: loss nan\n",
      "iteration 2600 / 7000: loss nan\n",
      "iteration 2700 / 7000: loss nan\n",
      "iteration 2800 / 7000: loss nan\n",
      "iteration 2900 / 7000: loss nan\n",
      "iteration 3000 / 7000: loss nan\n",
      "iteration 3100 / 7000: loss nan\n",
      "iteration 3200 / 7000: loss nan\n",
      "iteration 3300 / 7000: loss nan\n",
      "iteration 3400 / 7000: loss nan\n",
      "iteration 3500 / 7000: loss nan\n",
      "iteration 3600 / 7000: loss nan\n",
      "iteration 3700 / 7000: loss nan\n",
      "iteration 3800 / 7000: loss nan\n",
      "iteration 3900 / 7000: loss nan\n",
      "iteration 4000 / 7000: loss nan\n",
      "iteration 4100 / 7000: loss nan\n",
      "iteration 4200 / 7000: loss nan\n",
      "iteration 4300 / 7000: loss nan\n",
      "iteration 4400 / 7000: loss nan\n",
      "iteration 4500 / 7000: loss nan\n",
      "iteration 4600 / 7000: loss nan\n",
      "iteration 4700 / 7000: loss nan\n",
      "iteration 4800 / 7000: loss nan\n",
      "iteration 4900 / 7000: loss nan\n",
      "iteration 5000 / 7000: loss nan\n",
      "iteration 5100 / 7000: loss nan\n",
      "iteration 5200 / 7000: loss nan\n",
      "iteration 5300 / 7000: loss nan\n",
      "iteration 5400 / 7000: loss nan\n",
      "iteration 5500 / 7000: loss nan\n",
      "iteration 5600 / 7000: loss nan\n",
      "iteration 5700 / 7000: loss nan\n",
      "iteration 5800 / 7000: loss nan\n",
      "iteration 5900 / 7000: loss nan\n",
      "iteration 6000 / 7000: loss nan\n",
      "iteration 6100 / 7000: loss nan\n",
      "iteration 6200 / 7000: loss nan\n",
      "iteration 6300 / 7000: loss nan\n",
      "iteration 6400 / 7000: loss nan\n",
      "iteration 6500 / 7000: loss nan\n",
      "iteration 6600 / 7000: loss nan\n",
      "iteration 6700 / 7000: loss nan\n",
      "iteration 6800 / 7000: loss nan\n",
      "iteration 6900 / 7000: loss nan\n",
      "new acc  0.087\n",
      "lr, rs 0.005 15000.0\n",
      "lr 3.000000e-08 reg 1.000000e+03 train accuracy: 0.102143 val accuracy: 0.105000\n",
      "lr 3.000000e-08 reg 2.000000e+03 train accuracy: 0.095163 val accuracy: 0.089000\n",
      "lr 3.000000e-08 reg 1.500000e+04 train accuracy: 0.248592 val accuracy: 0.255000\n",
      "lr 5.000000e-07 reg 1.000000e+03 train accuracy: 0.245918 val accuracy: 0.261000\n",
      "lr 5.000000e-07 reg 2.000000e+03 train accuracy: 0.245306 val accuracy: 0.262000\n",
      "lr 5.000000e-07 reg 1.500000e+04 train accuracy: 0.241918 val accuracy: 0.253000\n",
      "lr 1.000000e-06 reg 1.000000e+03 train accuracy: 0.244061 val accuracy: 0.254000\n",
      "lr 1.000000e-06 reg 2.000000e+03 train accuracy: 0.246327 val accuracy: 0.262000\n",
      "lr 1.000000e-06 reg 1.500000e+04 train accuracy: 0.243429 val accuracy: 0.262000\n",
      "lr 5.000000e-06 reg 1.000000e+03 train accuracy: 0.246959 val accuracy: 0.260000\n",
      "lr 5.000000e-06 reg 2.000000e+03 train accuracy: 0.241061 val accuracy: 0.260000\n",
      "lr 5.000000e-06 reg 1.500000e+04 train accuracy: 0.243061 val accuracy: 0.252000\n",
      "lr 1.000000e-05 reg 1.000000e+03 train accuracy: 0.248020 val accuracy: 0.257000\n",
      "lr 1.000000e-05 reg 2.000000e+03 train accuracy: 0.241163 val accuracy: 0.253000\n",
      "lr 1.000000e-05 reg 1.500000e+04 train accuracy: 0.241551 val accuracy: 0.257000\n",
      "lr 5.000000e-05 reg 1.000000e+03 train accuracy: 0.254510 val accuracy: 0.262000\n",
      "lr 5.000000e-05 reg 2.000000e+03 train accuracy: 0.238571 val accuracy: 0.249000\n",
      "lr 5.000000e-05 reg 1.500000e+04 train accuracy: 0.179571 val accuracy: 0.182000\n",
      "lr 5.000000e-04 reg 1.000000e+03 train accuracy: 0.212612 val accuracy: 0.207000\n",
      "lr 5.000000e-04 reg 2.000000e+03 train accuracy: 0.091490 val accuracy: 0.110000\n",
      "lr 5.000000e-04 reg 1.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 2.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-03 reg 1.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 2.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-03 reg 1.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.262000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from deep_learning_su.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [3e-8, 5e-7, 1e-6, 5e-6, 1e-5, 5e-5, 5e-4, 1e-3, 5e-3]\n",
    "regularization_strengths = [1e3,2e3, 1.5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "'''\n",
    "num_rates = 5\n",
    "rates_start = learning_rates[0]\n",
    "rates_stop = learning_rates[1]\n",
    "learning_rates = np.arange(rates_start, rates_stop, (rates_stop - rates_start)/(num_rates))\n",
    "\n",
    "num_strengths = 4\n",
    "strengths_start = regularization_strengths[0]\n",
    "strengths_stop = regularization_strengths[1]\n",
    "regularization_strengths = np.arange(strengths_start, strengths_stop, (strengths_stop-strengths_start)/(num_strengths))\n",
    "'''\n",
    "print(\"learning rates \", learning_rates)\n",
    "for lr in learning_rates:\n",
    "    for rs in regularization_strengths:\n",
    "        \n",
    "        softmax = Softmax()\n",
    "        softmax.train(X_train, y_train, lr, rs, num_iters=7000, verbose=True)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        eval_acc = np.mean(np.equal(y_val_pred, y_val))\n",
    "        \n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        train_acc =np.mean(np.equal(y_train_pred, y_train))\n",
    "        \n",
    "        results[(lr, rs)] = (train_acc, eval_acc)\n",
    "        print(\"new acc \", eval_acc)\n",
    "        print(\"lr, rs\", lr, rs)\n",
    "        if eval_acc > best_val:\n",
    "            \n",
    "            best_val = eval_acc\n",
    "            best_softmax = softmax\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.261000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
